{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Data Extraction\n",
    "Monthly data extraction from Twitter API guided by the following [plan](https://docs.google.com/document/d/1d_4WeDetmZUkk9JJUEWiqqZaBaFsxy1ZDFNiL0JVaok/edit?usp=sharing) | [ISP Selection Guidelines](https://docs.google.com/document/d/12n9hZNdCLmrIVfK05MCa1CUhEYoCR9Ib0fPnxQZql_E/edit?usp=sharing) | [Twitter API Operators](https://developer.twitter.com/en/docs/twitter-api/tweets/search/integrate/build-a-query#limits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import relevant libraries\n",
    "import tweepy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. SETTING UP & CONNECTING TO THE API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def authenticate():\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to handle API connection, setup and authentication\n",
    "    \"\"\"\n",
    "    \n",
    "    #Import the twitter credentials stored in a separate file\n",
    "    %run ../src/credentials/twitter_credentials\n",
    "    \n",
    "    #Create the authentication object\n",
    "    auth = tweepy.OAuthHandler(api_key,api_secret_key)\n",
    "\n",
    "    #Set the access token and access token secret\n",
    "    auth.set_access_token(access_token,access_token_secret)\n",
    "\n",
    "    #Create the API object\n",
    "    api = tweepy.API(auth)  \n",
    "    \n",
    "    return api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. SPECIFYING VARIABLES FOR THE DATA EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dev environment for the full archive endpoint (allows access to all tweets overtime)\n",
    "dev_env = 'prod'\n",
    "\n",
    "#THE BELOW SPECIFY HANDLES MANAGED BY THE ISPs or HANDLES THAT APPEAR TO TWEET BRAND PROMOTIONAL CONTENT\n",
    "#TWEETS FROM THESE HANDLES WILL BE AVOIDED WHEN EXTRACTING\n",
    "#Note: It is infeasible to address all cases. However, we would expect such tweets to be in the minority\n",
    "\n",
    "#Spectranet ISP\n",
    "spectranet_handles = ['-from:spectranet_NG','Spectr_net','SPECTRANETLTE','spectranet__NG']\n",
    "\n",
    "#IPNX ISP\n",
    "ipnx_handles = ['-from:ipNXTweet','IpnxSupport','iRecruite']\n",
    "\n",
    "#Tizeti (Wifi.ng) ISP\n",
    "tizeti_handles = ['-from:tizeti','wifisupport1']\n",
    "\n",
    "#Cobranet ISP\n",
    "cobranet_handles = ['-from:Cobranetisp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Extracting the Tweets\n",
    "\n",
    "#### Splitting the yearly quarters from which data will be extracted into subintervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_range(start, end, intv):\n",
    "    \n",
    "    \"\"\"\n",
    "    Split Date Range into Roughly Equal Sub Intervals. Adapted from StackOverflow answer by Abhijit(2015)\n",
    "    Retrieved from https://stackoverflow.com/questions/29721228\n",
    "    \n",
    "    Inputs\n",
    "        - start (str): The start date of the time period\n",
    "        - end (str): The end date of the time period\n",
    "        - intv (int): Interval size (i.e. split the duration into roughly 'intv' equal subintervals)\n",
    "        \n",
    "    Outputs\n",
    "        - Generator object containing the subinterval dates\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #Convert start date to datetime object\n",
    "    start = datetime.strptime(start,\"%Y%m%d\")\n",
    "    \n",
    "    #Convert end date to datetime object\n",
    "    end = datetime.strptime(end,\"%Y%m%d\")\n",
    "    \n",
    "    #Find the roughly equal subinterval length\n",
    "    diff = (end  - start ) / intv\n",
    "    \n",
    "    #Compute the subinterval dates and yield as string\n",
    "    for i in range(intv):\n",
    "        \n",
    "        #After the first sub interval, start intervals from the day after the last interval's end day\n",
    "        if i > 1:\n",
    "            yield (start + diff * (i-1) + timedelta(1)).strftime(\"%Y%m%d\")\n",
    "        yield (start + diff * i).strftime(\"%Y%m%d\")\n",
    "        \n",
    "    #Compute the last interval\n",
    "    yield (start + diff * (intv-1) + timedelta(1)).strftime(\"%Y%m%d\")\n",
    "    yield end.strftime(\"%Y%m%d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specifying the query & pulling from the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def ISP_Tweet_Extractor(api,isp_name, from_date, to_date):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to extract tweets for a specified ISP during a specified time frame\n",
    "    \n",
    "    Inputs:\n",
    "     - isp_name (str): Name of the ISP to extract tweets for\n",
    "     - from_date (str): Earliest date (and time) of posting for any extracted tweet\n",
    "     - to_date (str): Latest date (and time) of posting for any extracted tweet \n",
    "     \n",
    "    Output:\n",
    "     - subintv_ISP_tweets (list): List containing API results for yearly quarter subintervals\n",
    "     \n",
    "    \"\"\"\n",
    "    \n",
    "    #Connection to api\n",
    "    api = api\n",
    "    \n",
    "    #### ------------------ VARIABLE REFORMATTING ------------------ ####\n",
    "    #Assign ISP name to variable ensuring it is in lower case\n",
    "    isp_name = isp_name.lower()\n",
    "    \n",
    "    #Reformat the fromDate to YYYYMMDD format\n",
    "    from_date = from_date.replace('-','')\n",
    "    \n",
    "    #Reformat the fromDate to YYYYMMDD format\n",
    "    to_date = to_date.replace('-','')\n",
    "    \n",
    "    #Split the quarter (from_date - to_date) to 5 equal subintervals\n",
    "    #*100 tweets will be extracted from each subinterval\n",
    "    intv_dates = [*date_range(from_date, to_date, 5)]\n",
    "    \n",
    "    #Get the subinterval date pairs\n",
    "    date_pairs = [(intv_dates[idx],intv_dates[idx+1]) for idx in range(0,len(intv_dates),2)]\n",
    "    \n",
    "    \n",
    "    #### ------------------ BUILDING THE API QUERY  ------------------ ####\n",
    "    \n",
    "    #Join the different handles to form the exclusion portion of the query\n",
    "    excl_handles = ' -from:'.join(eval(isp_name +'_handles'))\n",
    "    \n",
    "    #Query for tweets in Lagos containing the ISP's name and exclude tweets \n",
    "    #from the official ISP Twitter handles\n",
    "    \n",
    "    #If the ISP is Tizeti, take into account that they are known by multiple names\n",
    "    if isp_name == 'tizeti':\n",
    "        \n",
    "        api_query = f\"\"\" tizeti OR wifi.com.ng OR wifi.ng {excl_handles} place:\"Lagos,Nigeria\" \"\"\"\n",
    "    \n",
    "    else:\n",
    "        api_query = f\"\"\" {isp_name} {excl_handles} place:\"Lagos,Nigeria\" \"\"\"\n",
    "        \n",
    "        \n",
    "    #### ------------------ SEARCHING & EXTRACTING THE DATA ------------------ ####\n",
    "    \n",
    "    #List to store the subinterval API responses\n",
    "    subintv_ISP_tweets = []\n",
    "    \n",
    "    #For each subinterval\n",
    "    for start,end in date_pairs:\n",
    "        \n",
    "        #Add time to the dates to fit with Twitter API format, \n",
    "        start = start + '0000' #midnight\n",
    "        end = end + '2359' #just before crossing into the next day\n",
    "        \n",
    "        #Trying running the query\n",
    "        try:\n",
    "            #Full archive search for ISP tweets\n",
    "            ISP_tweets = api.search_full_archive(dev_env, api_query, fromDate = start, toDate= end)\n",
    "        \n",
    "        #If it fails, print the exception raised and the subinterval in question, but continue\n",
    "        except Exception as e:\n",
    "            \n",
    "            print(e,'\\n')\n",
    "            print(f'Subinterval associated with error: [{start},{end}]')\n",
    "            continue\n",
    "            \n",
    "        #Add the subinterval API response to the list\n",
    "        subintv_ISP_tweets.append(ISP_tweets)\n",
    "    \n",
    "    \n",
    "    return subintv_ISP_tweets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Storing the tweets in a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweets_to_df(api_result_list):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to extract relevant properties from api results (tweets objects) and store \n",
    "    in a pandas dataframe\n",
    "    \n",
    "    Input(s):\n",
    "        - api_result_list (list): List containing API results for a yearly quarter's subintervals\n",
    "        \n",
    "    Output(s):\n",
    "        - main_df (DataFrame): Pandas DataFrame of tweets (and their properties) from the yearly quarter\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #Empty dataframe to compile data from all yearly quarter subintervals\n",
    "    main_df = pd.DataFrame()\n",
    "    \n",
    "    #Iterate through all the subinterval api results\n",
    "    for api_result in api_result_list:\n",
    "        \n",
    "        #List to store the tweets from each subinterval api_result\n",
    "        tweets = []\n",
    "        \n",
    "        #Iterate through all the tweets\n",
    "        for tweet in api_result:\n",
    "            \n",
    "            #Dictionary to store tweet properties\n",
    "            tweet_prop = {}\n",
    "            \n",
    "            #Store the tweet time\n",
    "            tweet_prop['Time'] = tweet.created_at\n",
    "            \n",
    "            #Store the tweet text – ensuring that the full text is gotten (if truncated)\n",
    "            if tweet.truncated:\n",
    "                tweet_prop['Text'] = tweet.extended_tweet['full_text']\n",
    "            else:\n",
    "                tweet_prop['Text'] = tweet.text\n",
    "                \n",
    "            #Store the coordinates (if available)\n",
    "            tweet_prop['Coordinates'] = tweet.coordinates\n",
    "            \n",
    "            #Store the place\n",
    "            tweet_prop['Place'] = tweet.place\n",
    "            \n",
    "            #Store the source (e.g. Android, iOS, Desktop)\n",
    "            tweet_prop['Source'] = tweet.source\n",
    "            \n",
    "            #Store the tweet in the tweets list\n",
    "            tweets.append(tweet_prop)\n",
    "    \n",
    "        #Convert the dictionary to a pandas dataframe\n",
    "        df = pd.DataFrame.from_dict(tweets)\n",
    "        \n",
    "        #Append the pandas df for the API result to the main df\n",
    "        main_df = main_df.append(df)\n",
    "        \n",
    "    \n",
    "    return main_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting pandas df to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_csv(df,isp_name,from_date,yearly_quarter):\n",
    "    \n",
    "    #Alphanumerics to lowercase\n",
    "    isp_name = isp_name.lower()\n",
    "    quarter = yearly_quarter.lower()\n",
    "    \n",
    "    #Extract year from date\n",
    "    year = from_date[:4]\n",
    "    \n",
    "    #Convert to CSV to save current tweets obtained from the API\n",
    "    df.to_csv(f\"../data/raw/{isp_name}/{isp_name}_tweets_{quarter}_{year}.csv\", index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(isp_name=None, from_date=None, to_date=None, yearly_quarter=None, interactive=False):\n",
    "    \n",
    "    if interactive:\n",
    "        #Pass in parameters neeeded for API query\n",
    "        isp_name = input('ISP Full Name:')\n",
    "        from_date = input('Start Date (YYYY-MM-DD):')\n",
    "        to_date = input('End Date (YYYY-MM-DD):')\n",
    "        yearly_quarter = input('What quarter of the year? (q_):')\n",
    "        \n",
    "    else:\n",
    "        if any(x is None for x in [isp_name,from_date,to_date,yearly_quarter]):\n",
    "            \n",
    "            raise ValueError('Please ensure a valid value is passed for all the parameters')\n",
    "    \n",
    "    #Connect and authenticate Twitter API\n",
    "    api = authenticate()\n",
    "    \n",
    "    #Pull the data from the API using the query and parameters\n",
    "    api_results = ISP_Tweet_Extractor(api, isp_name, from_date, to_date)\n",
    "    \n",
    "    #Convert the API results into a pandas dataframe\n",
    "    ISP_tweets = tweets_to_df(api_results)\n",
    "    \n",
    "    #Write to csv file\n",
    "    df_to_csv(ISP_tweets,isp_name,from_date,yearly_quarter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Code [Option 1]: Pass in arguments in list to run\n",
    "`Non-interactive` – Uncomment below cell (it executes extraction on running!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "quarterly_dates_2019 = [('2019-01-01','2019-03-31','q1'),('2019-04-01','2019-06-30','q2'),\n",
    "                        ('2019-07-01','2019-09-30','q3'),('2019-10-01','2019-12-31','q4')]\n",
    "\n",
    "quarterly_dates_2020 = [('2020-01-01','2020-03-31','q1'),('2020-04-01','2020-06-30','q2'),\n",
    "                        ('2020-07-01','2020-09-30','q3'),('2020-10-01','2020-12-31','q4')]\n",
    "\n",
    "for start, end, quarter in quarterly_dates_2020:\n",
    "    \n",
    "    if __name__ == \"__main__\":\n",
    "        main('spectranet',start,end,quarter)\n",
    "    time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Code [Option 2]: Pass in arguments one after the other\n",
    "`Interactive`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ISP Full Name: tizeti\n",
      "Start Date (YYYY-MM-DD): 2019-01-01\n",
      "End Date (YYYY-MM-DD): 2019-03-31\n",
      "What quarter of the year? (q_): q1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "422 Unprocessable Entity\n",
      "There were errors processing your request: Reference to invalid operator 'is:retweet'. Operator is not available in current product or product packaging. Please refer to complete available operator list at http://t.co/operators. (at position 92) \n",
      "\n",
      "Subinterval associated with error: [201901010000,201901182359]\n",
      "422 Unprocessable Entity\n",
      "There were errors processing your request: Reference to invalid operator 'is:retweet'. Operator is not available in current product or product packaging. Please refer to complete available operator list at http://t.co/operators. (at position 92) \n",
      "\n",
      "Subinterval associated with error: [201901190000,201902052359]\n",
      "422 Unprocessable Entity\n",
      "There were errors processing your request: Reference to invalid operator 'is:retweet'. Operator is not available in current product or product packaging. Please refer to complete available operator list at http://t.co/operators. (at position 92) \n",
      "\n",
      "Subinterval associated with error: [201902060000,201902232359]\n",
      "422 Unprocessable Entity\n",
      "There were errors processing your request: Reference to invalid operator 'is:retweet'. Operator is not available in current product or product packaging. Please refer to complete available operator list at http://t.co/operators. (at position 92) \n",
      "\n",
      "Subinterval associated with error: [201902240000,201903132359]\n",
      "422 Unprocessable Entity\n",
      "There were errors processing your request: Reference to invalid operator 'is:retweet'. Operator is not available in current product or product packaging. Please refer to complete available operator list at http://t.co/operators. (at position 92) \n",
      "\n",
      "Subinterval associated with error: [201903140000,201903312359]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main(interactive=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
