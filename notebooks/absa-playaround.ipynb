{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "232487df-7549-4687-9269-aede5d3e0b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/koredeakande/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f41d1d1-e352-4955-bab9-87f5a1dbca60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet_ic to\n",
      "[nltk_data]     /Users/koredeakande/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet_ic.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet_ic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0eb98231-329c-4ee4-965d-e732964748ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from itertools import product\n",
    "\n",
    "wordx, wordy = \"customer-service\",\"customer-care\"\n",
    "sem1, sem2 = wn.synsets(wordx), wn.synsets(wordy)\n",
    "\n",
    "maxscore = 0\n",
    "for i,j in list(product(*[sem1,sem2])):\n",
    "  score = i.wup_similarity(j) # Wu-Palmer Similarity\n",
    "  maxscore = score if maxscore < score else maxscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d05e40ed-fcdd-4274-ba4a-bf9400691571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4e7d43d9-f9f1-4741-b195-41173b121546",
   "metadata": {},
   "outputs": [
    {
     "ename": "WordNetError",
     "evalue": "Computing the least common subsumer requires Synset('speed.n.01') and Synset('decelerate.v.01') to have the same part of speech.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mWordNetError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/j5/540q0bw12gx3g56qg4llbmlh0000gn/T/ipykernel_7281/2298137082.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmaxscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproduct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msem1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msem2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjcn_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwordnet_ic\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Wu-Palmer Similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0;31m#print(i,j,score)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mmaxscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmaxscore\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaxscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/jupyterlab/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36mjcn_similarity\u001b[0;34m(self, other, ic, verbose)\u001b[0m\n\u001b[1;32m   1015\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_INF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1017\u001b[0;31m         \u001b[0mic1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mic2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlcs_ic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_lcs_ic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m         \u001b[0;31m# If either of the input synsets are the root synset, or have a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/jupyterlab/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36m_lcs_ic\u001b[0;34m(synset1, synset2, ic, verbose)\u001b[0m\n\u001b[1;32m   2236\u001b[0m     \"\"\"\n\u001b[1;32m   2237\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msynset1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pos\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0msynset2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2238\u001b[0;31m         raise WordNetError(\n\u001b[0m\u001b[1;32m   2239\u001b[0m             \u001b[0;34m\"Computing the least common subsumer requires \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2240\u001b[0m             \u001b[0;34m\"%s and %s to have the same part of speech.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msynset1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynset2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mWordNetError\u001b[0m: Computing the least common subsumer requires Synset('speed.n.01') and Synset('decelerate.v.01') to have the same part of speech."
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet_ic\n",
    "\n",
    "wordx, wordy = \"speed\",\"slow\"\n",
    "sem1, sem2 = wn.synsets(wordx), wn.synsets(wordy)\n",
    "\n",
    "maxscore = 0\n",
    "for i,j in list(product(*[sem1,sem2])):\n",
    "  score = i.jcn_similarity(j,wordnet_ic) # Wu-Palmer Similarity\n",
    "  #print(i,j,score)\n",
    "  maxscore = score if maxscore < score else maxscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16c61dbf-23d3-4017-a3cf-1f29227e3d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4444444444444444"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "296213ee-1bf4-407c-b050-a24300b90019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement difflib (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for difflib\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install difflib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "eff6ab57-86c2-4e62-b863-59b402b5d302",
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a7bb9e39-0060-4c47-b907-6a14f100e802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      slow speed       0.222222  \n",
      "      fast speed       0.222222  \n",
      "     snail speed       0.200000  \n"
     ]
    }
   ],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "interests = ('slow','fast','snail')\n",
    "keyword = ('speed')\n",
    "\n",
    "s = SequenceMatcher(None)\n",
    "\n",
    "limit = 0.50\n",
    "\n",
    "for interest in interests:\n",
    "    s.set_seq2(interest)\n",
    "    \n",
    "    s.set_seq1(keyword)\n",
    "    b = s.ratio()>=limit and len(s.get_matching_blocks())==2\n",
    "    print ('%10s %-10s  %f  %s' % (interest, keyword,\n",
    "                                  s.ratio(),\n",
    "                                  '** MATCH **' if b else ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c122f10b-f517-490e-bdce-4cc09384fba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "s.set_seq1(keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf00b23-823c-44ff-8eee-b3da108aedb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conceptually, TextWiser is composed of an Embedding, potentially with a pretrained model,\n",
    "# that can be chained into zero or more Transformations\n",
    "from textwiser import TextWiser, Embedding, Transformation, WordOptions, PoolOptions\n",
    "\n",
    "# Data\n",
    "documents = [\"Some document\", \"More documents. Including multi-sentence documents.\"]\n",
    "\n",
    "# Model: TFIDF `min_df` parameter gets passed to sklearn automatically\n",
    "emb = TextWiser(Embedding.TfIdf(min_df=1))\n",
    "\n",
    "# Model: TFIDF followed with an NMF + SVD\n",
    "emb = TextWiser(Embedding.TfIdf(min_df=1), [Transformation.NMF(n_components=30), Transformation.SVD(n_components=10)])\n",
    "\n",
    "# Model: Word2Vec with no pretraining that learns from the input data\n",
    "emb = TextWiser(Embedding.Word(word_option=WordOptions.word2vec, pretrained=None), Transformation.Pool(pool_option=PoolOptions.min))\n",
    "\n",
    "# Model: BERT with the pretrained bert-base-uncased embedding\n",
    "emb = TextWiser(Embedding.Word(word_option=WordOptions.bert), Transformation.Pool(pool_option=PoolOptions.first))\n",
    "\n",
    "# Features\n",
    "vecs = emb.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037a9e58-f596-42d3-9665-633c71e30836",
   "metadata": {},
   "outputs": [],
   "source": [
    "python train.py --model_name bert_spc --dataset twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfc3b5a2-8655-4db5-9fcb-8e753d92d7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37e87fe1-2db0-4d52-a1ca-2f3318876ab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mweight\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msize_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mignore_index\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreduction\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'mean'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "This criterion computes the cross entropy loss between input and target.\n",
       "\n",
       "It is useful when training a classification problem with `C` classes.\n",
       "If provided, the optional argument :attr:`weight` should be a 1D `Tensor`\n",
       "assigning weight to each of the classes.\n",
       "This is particularly useful when you have an unbalanced training set.\n",
       "\n",
       "The `input` is expected to contain raw, unnormalized scores for each class.\n",
       "`input` has to be a Tensor of size either :math:`(minibatch, C)` or\n",
       ":math:`(minibatch, C, d_1, d_2, ..., d_K)` with :math:`K \\geq 1` for the\n",
       "`K`-dimensional case. The latter is useful for higher dimension inputs, such\n",
       "as computing cross entropy loss per-pixel for 2D images.\n",
       "\n",
       "The `target` that this criterion expects should contain either:\n",
       "\n",
       "- Class indices in the range :math:`[0, C-1]` where :math:`C` is the number of classes; if\n",
       "  `ignore_index` is specified, this loss also accepts this class index (this index\n",
       "  may not necessarily be in the class range). The unreduced (i.e. with :attr:`reduction`\n",
       "  set to ``'none'``) loss for this case can be described as:\n",
       "\n",
       "  .. math::\n",
       "      \\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\n",
       "      l_n = - w_{y_n} \\log \\frac{\\exp(x_{n,y_n})}{\\sum_{c=1}^C \\exp(x_{n,c})}\n",
       "      \\cdot \\mathbb{1}\\{y_n \\not= \\text{ignore\\_index}\\}\n",
       "\n",
       "  where :math:`x` is the input, :math:`y` is the target, :math:`w` is the weight,\n",
       "  :math:`C` is the number of classes, and :math:`N` spans the minibatch dimension as well as\n",
       "  :math:`d_1, ..., d_k` for the `K`-dimensional case. If\n",
       "  :attr:`reduction` is not ``'none'`` (default ``'mean'``), then\n",
       "\n",
       "  .. math::\n",
       "      \\ell(x, y) = \\begin{cases}\n",
       "          \\sum_{n=1}^N \\frac{1}{\\sum_{n=1}^N w_{y_n} \\cdot \\mathbb{1}\\{y_n \\not= \\text{ignore\\_index}\\}} l_n, &\n",
       "           \\text{if reduction} = \\text{`mean';}\\\\\n",
       "            \\sum_{n=1}^N l_n,  &\n",
       "            \\text{if reduction} = \\text{`sum'.}\n",
       "        \\end{cases}\n",
       "\n",
       "  Note that this case is equivalent to the combination of :class:`~torch.nn.LogSoftmax` and\n",
       "  :class:`~torch.nn.NLLLoss`.\n",
       "\n",
       "- Probabilities for each class; useful when labels beyond a single class per minibatch item\n",
       "  are required, such as for blended labels, label smoothing, etc. The unreduced (i.e. with\n",
       "  :attr:`reduction` set to ``'none'``) loss for this case can be described as:\n",
       "\n",
       "  .. math::\n",
       "      \\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\n",
       "      l_n = - \\sum_{c=1}^C w_c \\log \\frac{\\exp(x_{n,c})}{\\exp(\\sum_{i=1}^C x_{n,i})} y_{n,c}\n",
       "\n",
       "  where :math:`x` is the input, :math:`y` is the target, :math:`w` is the weight,\n",
       "  :math:`C` is the number of classes, and :math:`N` spans the minibatch dimension as well as\n",
       "  :math:`d_1, ..., d_k` for the `K`-dimensional case. If\n",
       "  :attr:`reduction` is not ``'none'`` (default ``'mean'``), then\n",
       "\n",
       "  .. math::\n",
       "      \\ell(x, y) = \\begin{cases}\n",
       "          \\frac{\\sum_{n=1}^N l_n}{N}, &\n",
       "           \\text{if reduction} = \\text{`mean';}\\\\\n",
       "            \\sum_{n=1}^N l_n,  &\n",
       "            \\text{if reduction} = \\text{`sum'.}\n",
       "        \\end{cases}\n",
       "\n",
       ".. note::\n",
       "    The performance of this criterion is generally better when `target` contains class\n",
       "    indices, as this allows for optimized computation. Consider providing `target` as\n",
       "    class probabilities only when a single class label per minibatch item is too restrictive.\n",
       "\n",
       "Args:\n",
       "    weight (Tensor, optional): a manual rescaling weight given to each class.\n",
       "        If given, has to be a Tensor of size `C`\n",
       "    size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n",
       "        the losses are averaged over each loss element in the batch. Note that for\n",
       "        some losses, there are multiple elements per sample. If the field :attr:`size_average`\n",
       "        is set to ``False``, the losses are instead summed for each minibatch. Ignored\n",
       "        when :attr:`reduce` is ``False``. Default: ``True``\n",
       "    ignore_index (int, optional): Specifies a target value that is ignored\n",
       "        and does not contribute to the input gradient. When :attr:`size_average` is\n",
       "        ``True``, the loss is averaged over non-ignored targets. Note that\n",
       "        :attr:`ignore_index` is only applicable when the target contains class indices.\n",
       "    reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n",
       "        losses are averaged or summed over observations for each minibatch depending\n",
       "        on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n",
       "        batch element instead and ignores :attr:`size_average`. Default: ``True``\n",
       "    reduction (string, optional): Specifies the reduction to apply to the output:\n",
       "        ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will\n",
       "        be applied, ``'mean'``: the weighted mean of the output is taken,\n",
       "        ``'sum'``: the output will be summed. Note: :attr:`size_average`\n",
       "        and :attr:`reduce` are in the process of being deprecated, and in\n",
       "        the meantime, specifying either of those two args will override\n",
       "        :attr:`reduction`. Default: ``'mean'``\n",
       "    label_smoothing (float, optional): A float in [0.0, 1.0]. Specifies the amount\n",
       "        of smoothing when computing the loss, where 0.0 means no smoothing. The targets\n",
       "        become a mixture of the original ground truth and a uniform distribution as described in\n",
       "        `Rethinking the Inception Architecture for Computer Vision <https://arxiv.org/abs/1512.00567>`__. Default: :math:`0.0`.\n",
       "\n",
       "Shape:\n",
       "    - Input: :math:`(N, C)` where `C = number of classes`, or\n",
       "      :math:`(N, C, d_1, d_2, ..., d_K)` with :math:`K \\geq 1`\n",
       "      in the case of `K`-dimensional loss.\n",
       "    - Target: If containing class indices, shape :math:`(N)` where each value is\n",
       "      :math:`0 \\leq \\text{targets}[i] \\leq C-1`, or :math:`(N, d_1, d_2, ..., d_K)` with\n",
       "      :math:`K \\geq 1` in the case of K-dimensional loss. If containing class probabilities,\n",
       "      same shape as the input.\n",
       "    - Output: If :attr:`reduction` is ``'none'``, shape :math:`(N)` or\n",
       "      :math:`(N, d_1, d_2, ..., d_K)` with :math:`K \\geq 1` in the case of K-dimensional loss.\n",
       "      Otherwise, scalar.\n",
       "\n",
       "Examples::\n",
       "\n",
       "    >>> # Example of target with class indices\n",
       "    >>> loss = nn.CrossEntropyLoss()\n",
       "    >>> input = torch.randn(3, 5, requires_grad=True)\n",
       "    >>> target = torch.empty(3, dtype=torch.long).random_(5)\n",
       "    >>> output = loss(input, target)\n",
       "    >>> output.backward()\n",
       "    >>>\n",
       "    >>> # Example of target with class probabilities\n",
       "    >>> input = torch.randn(3, 5, requires_grad=True)\n",
       "    >>> target = torch.randn(3, 5).softmax(dim=1)\n",
       "    >>> output = loss(input, target)\n",
       "    >>> output.backward()\n",
       "\u001b[0;31mInit docstring:\u001b[0m Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/opt/anaconda3/envs/jupyterlab/lib/python3.9/site-packages/torch/nn/modules/loss.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?nn.CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c92a4d4-34e7-4993-98a1-7166495aa513",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
