{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fba88215-aab4-4b5a-b8f5-80fc0d628f93",
   "metadata": {},
   "source": [
    "# Modeling: Aspect-Based Sentiment Analysis [Simplistic]\n",
    "\n",
    "**`Goal:`** \n",
    "\n",
    "Conduct ABSA using word relatedness and out-of-the-box [ABSA package by ScalaConsultants](https://github.com/ScalaConsultants/Aspect-Based-Sentiment-Analysis). This notebook is meant to serve as a start for tweet aspect annotation by getting as much of the aspects indicated and their corresponding sentiments. \n",
    "\n",
    "**Note:** Results will be crosschecked during the annotation phase!\n",
    "\n",
    "**`Process:`** \n",
    "1. List aspects (e.g. speed, price, reliability) determined from earlier data annotation phase\n",
    "2. Get nouns, adjectives and adverbs from the tweets as these will likely be the parts of speech making meaningful reference to aspects\n",
    "3. Check if each of the words from step 2 is very similar to any of the aspects (e.g. speed [aspect] and fast [word in tweet]) by computing relatedness score (via word embedding)\n",
    "4. If relatedness score is past a set thresholdhood, we assume the aspect was referenced in the tweet. Hence, note down that the aspect category was referenced in that given tweet and also note down the word (herein called aspect term) that implied the aspect\n",
    "6. Conduct ABSA using the ABSA package with the tweet and with the aspect term and note sentiment (positive, negative or neutral) towards the main aspect (price, speed, etc.)\n",
    "7. If multiple words make reference to a single aspect, find the average of their sentiments and use to assign a single sentiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d062c92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python -m spacy download en_core_web_md\n",
    "# python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c11b87e-67cd-44e7-a51c-e26a44c442f4",
   "metadata": {},
   "source": [
    "### 1. Library Importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ef903c6-788c-4790-ae9a-8e7eaf9dac83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-25 21:59:15.631180: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import aspect_based_sentiment_analysis as absa\n",
    "import nltk\n",
    "from nltk import pos_tag, RegexpParser\n",
    "\n",
    "#Packages for word relatedness computation\n",
    "import spacy\n",
    "spacy_nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "from itertools import product\n",
    "from cleantext import clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f196af6-46fc-46fc-b124-d0c0f8a490e6",
   "metadata": {},
   "source": [
    "### 2. Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d50c70f7-0099-4f15-8e56-74c646ee921f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/processed/sample_encoded_and_cleaned_no_punct.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72aa0c30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISP_Name</th>\n",
       "      <th>Time</th>\n",
       "      <th>Text</th>\n",
       "      <th>Source</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sprectranet</td>\n",
       "      <td>2020-02-04 18:30:35+00:00</td>\n",
       "      <td>my family used my spectranet and they dont wan...</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sprectranet</td>\n",
       "      <td>2019-06-19 04:59:49</td>\n",
       "      <td>spectranetng how can i get the freedom mifi in...</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sprectranet</td>\n",
       "      <td>2020-03-30 07:57:38+00:00</td>\n",
       "      <td>drolufunmilayo iconicremi spectranetng</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sprectranet</td>\n",
       "      <td>2020-12-31 21:07:52+00:00</td>\n",
       "      <td>spectranetng your response just proves how hor...</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sprectranet</td>\n",
       "      <td>2020-09-03 23:09:09+00:00</td>\n",
       "      <td>spectranet is just the worse tbh i cant even w...</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ISP_Name                       Time  \\\n",
       "0  sprectranet  2020-02-04 18:30:35+00:00   \n",
       "1  sprectranet        2019-06-19 04:59:49   \n",
       "2  sprectranet  2020-03-30 07:57:38+00:00   \n",
       "3  sprectranet  2020-12-31 21:07:52+00:00   \n",
       "4  sprectranet  2020-09-03 23:09:09+00:00   \n",
       "\n",
       "                                                Text               Source  \\\n",
       "0  my family used my spectranet and they dont wan...  Twitter for Android   \n",
       "1  spectranetng how can i get the freedom mifi in...   Twitter for iPhone   \n",
       "2             drolufunmilayo iconicremi spectranetng   Twitter for iPhone   \n",
       "3  spectranetng your response just proves how hor...  Twitter for Android   \n",
       "4  spectranet is just the worse tbh i cant even w...   Twitter for iPhone   \n",
       "\n",
       "  sentiment  label  \n",
       "0   Neutral      1  \n",
       "1   Neutral      1  \n",
       "2   Neutral      1  \n",
       "3  Negative      0  \n",
       "4  Negative      0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24e6e9d1-55fe-489a-bac7-4daad62c23c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Negative    216\n",
       "Neutral     131\n",
       "Positive     30\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b8d956e-53a3-4dc9-8da3-dfa6b60d3c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at absa/classifier-rest-0.2 were not used when initializing BertABSClassifier: ['dropout_379']\n",
      "- This IS expected if you are initializing BertABSClassifier from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertABSClassifier from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of BertABSClassifier were not initialized from the model checkpoint at absa/classifier-rest-0.2 and are newly initialized: ['dropout_37']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#Load the model for ABSA modeling\n",
    "nlp = absa.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5372bfb5-132c-41d5-9b84-05ae1dfa8974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('price', price),\n",
       " ('speed', speed),\n",
       " ('reliability', reliability),\n",
       " ('coverage', coverage),\n",
       " ('customer service', customer service),\n",
       " ('trustworthiness', trustworthiness)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1. List aspects determined during the annotation phase\n",
    "    #Note: This might not be exhaustive! But it should cover most cases. It is also subjective!\n",
    "    #Also using synonyms of these words will likely yield different results\n",
    "aspects = ['price','speed','reliability','coverage', 'customer service', 'trustworthiness']\n",
    "\n",
    "#2. Pair aspects with their tokenized form to avoid recomputation in the ABSA phase below\n",
    "aspects_with_token = [] #List to store the pairing\n",
    "\n",
    "#Iterate through the aspects and compute their word vector using spacy\n",
    "for aspect in aspects:\n",
    "    aspects_with_token.append((aspect,spacy_nlp(aspect)))\n",
    "    \n",
    "aspects_with_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "159d93a0-4570-41c9-9df1-663573e5f1e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/koredeakande/opt/anaconda3/envs/capstone/lib/python3.7/site-packages/ipykernel_launcher.py:111: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n"
     ]
    }
   ],
   "source": [
    "#Set to store all seen words\n",
    "seen_words = set()\n",
    "\n",
    "#Set to store all aspect implying words found – to avoid recomputing similarity scores\n",
    "aspect_implying_words_glob = set()\n",
    "\n",
    "#Dictionary categorizing all aspect-implying words into their relevant aspects\n",
    "aspects_with_implying_words = {'price':set(),'speed':set(),'reliability':set(),'coverage':set(), \n",
    "                               'customer service':set(),'trustworthiness':set()}\n",
    "\n",
    "#List to store detected aspects and their sentiments\n",
    "df_list = []\n",
    "\n",
    "#Similarity threshold\n",
    "sim_thresh = 0.6\n",
    "\n",
    "#Chunk tags to match – i.e. parts of speech to extract\n",
    "CHUNK_TAG = \"\"\"\n",
    "MATCH: {<NN>+|<NN.*>+}\n",
    "{<JJ.*>?}\n",
    "{<RB.*>?}\n",
    "\"\"\"\n",
    "\n",
    "#Initialize chunk tag parser\n",
    "cp = nltk.RegexpParser(CHUNK_TAG)\n",
    "\n",
    "#Iterate through all the tweets\n",
    "for tweet in df.Text:\n",
    "    \n",
    "    #Set to store the detected aspects at the sentence level\n",
    "    # detected_aspects = set()\n",
    "    \n",
    "    #Dictionary to store the sentiment value for each seen aspect\n",
    "    sentence_lvl_aspect_sentiment = {'price':[],'speed':[],'reliability':[],'coverage':[], \n",
    "                                     'customer service':[], 'trustworthiness':[]}\n",
    "        \n",
    "    #Split the tweet into words\n",
    "    text = tweet.split()\n",
    "\n",
    "    #Tag the words with their part of speech\n",
    "    tokens_tag = pos_tag(text)\n",
    "    \n",
    "    #Get the words with relevant POS (noun, adverbs, adjectives)\n",
    "    chunk_result = cp.parse(tokens_tag)\n",
    "    \n",
    "    #Extract chunk results from tree into list \n",
    "    chunk_items = [list(n) for n in chunk_result if isinstance(n, nltk.tree.Tree)]\n",
    "    \n",
    "    #Finally fuse/extract chunked words to get (noun) phrases, nouns, adverbs, adjectives\n",
    "    #1. List to store the words\n",
    "    matched_words = []\n",
    "    \n",
    "    #2. Iterate through the chunked words lists and get the relevant words\n",
    "    for item in chunk_items:\n",
    "        if len(item) > 1:\n",
    "            full_string = []\n",
    "\n",
    "            for word in item:\n",
    "                full_string.append(word[0])\n",
    "\n",
    "            matched_words.append(' '.join(full_string))\n",
    "\n",
    "        else:\n",
    "            matched_words.append(item[0][0])\n",
    "        \n",
    "    #Iterate through all the words\n",
    "    for word_in_focus in matched_words:\n",
    "        \n",
    "        #If the word has been seen before\n",
    "        if word_in_focus in seen_words:\n",
    "            \n",
    "            #Check if the word is an aspect-implying word\n",
    "            if word_in_focus in aspect_implying_words_glob:\n",
    "                \n",
    "                #List to store all the aspects found to related to the certain word/token\n",
    "                aspects_implied = []\n",
    "            \n",
    "                #If it is an aspect-implying word, iterate through all the aspects\n",
    "                for aspect in aspects_with_implying_words.keys():\n",
    "                    \n",
    "                    #Check if the word_in_focus was noted as a word implying the aspect\n",
    "                    if word_in_focus in aspects_with_implying_words[aspect]:\n",
    "                        \n",
    "                        #Get all the aspects the word_in_focus implies\n",
    "                        aspects_implied.append(aspect)\n",
    "                        \n",
    "            \n",
    "            else:\n",
    "                continue\n",
    "                    \n",
    "         \n",
    "        #If the word hasn't been seen before\n",
    "        else:\n",
    "            \n",
    "            #Mark the word as seen now\n",
    "            seen_words.add(word_in_focus)\n",
    "                \n",
    "            #List to store all the aspects found to related to the certain word/token\n",
    "            #Ideally a given word won't imply multiple of the aspects as they are fairly independent\n",
    "            #-but just in case \n",
    "            aspects_implied = []\n",
    "\n",
    "            #Iterate through all the aspects\n",
    "            for aspect,asp_token in aspects_with_token:\n",
    "\n",
    "                #Translate word_in_focus to word vector\n",
    "                spacy_token = spacy_nlp(word_in_focus)\n",
    "\n",
    "                #Compute the similarity between the two word vectors (i.e. the two words)\n",
    "                #Round up to 1 d.p.\n",
    "                similarity_score = round(asp_token.similarity(spacy_token),1)\n",
    "\n",
    "                #If the max similarity score seen is greater than the threshold\n",
    "                if similarity_score >= sim_thresh:\n",
    "\n",
    "                    #Add the word to the set of all aspect-implying words seen\n",
    "                    aspect_implying_words_glob.add(word_in_focus)\n",
    "\n",
    "                    #Add the word to the dictionary of the relevant aspect word\n",
    "                    aspects_with_implying_words[aspect].add(word_in_focus)\n",
    "\n",
    "                    #Note that the aspect has been found in this particular sentence\n",
    "                    # detected_aspects.add(aspect)\n",
    "\n",
    "                    #Add the aspect to the list of aspects that the word_in_focus implies\n",
    "                    aspects_implied.append(aspect)\n",
    "\n",
    "\n",
    "                #If the word is not an aspect implying word, continue to next word\n",
    "                else:\n",
    "\n",
    "                    continue\n",
    "                \n",
    "        #Calculate the sentiment scores for the aspect_implying word in the current sentence\n",
    "        sentiment = nlp(tweet ,aspects = [word_in_focus])\n",
    "        sentiment_scores = sentiment.subtasks[word_in_focus].examples[0].scores\n",
    "\n",
    "        #Note down the scores for all the implied aspects\n",
    "        for aspect in aspects_implied:\n",
    "            sentence_lvl_aspect_sentiment[aspect].append(sentiment_scores)\n",
    "    \n",
    "    #List to store the detected aspects from the sentence\n",
    "    detected_aspects = []\n",
    "    \n",
    "    #List to store the determined sentiments of the detected aspects\n",
    "    detected_sentiments = []\n",
    "    \n",
    "    #Iterate through all the aspects\n",
    "    for aspect in sentence_lvl_aspect_sentiment.keys():\n",
    "        \n",
    "        #If the aspect was detected in the sentence\n",
    "        if sentence_lvl_aspect_sentiment[aspect]:\n",
    "            \n",
    "            #Record this\n",
    "            detected_aspects.append(aspect)\n",
    "            \n",
    "            #Calculate the average sentiment scores across the different terms\n",
    "            avg_senti_score = np.array(sentence_lvl_aspect_sentiment[aspect]).mean(axis=0)\n",
    "            \n",
    "            #Get the sentiment category (neutral,negative,positive) with the largest probability\n",
    "            max_idx = np.argmax(avg_senti_score)\n",
    "\n",
    "            if max_idx == 2:\n",
    "\n",
    "                detected_sentiments.append(\"Positive\")\n",
    "\n",
    "            elif max_idx == 1:\n",
    "\n",
    "                detected_sentiments.append(\"Negative\")\n",
    "\n",
    "            else:\n",
    "\n",
    "                detected_sentiments.append(\"Neutral\")\n",
    "    \n",
    "    #Add the detected aspects and sentiments from the sentence to the list\n",
    "    if detected_aspects:\n",
    "        df_list.append([tweet,detected_aspects,detected_sentiments])\n",
    "    else:\n",
    "        df_list.append([tweet,None,None])\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea665475-4c9a-4a45-841f-c18421790031",
   "metadata": {},
   "outputs": [],
   "source": [
    "absa_df = pd.DataFrame(df_list, \n",
    "                       columns=['Tweets','Detected aspects','Corresponding sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b218bfb5-5716-4480-a9f0-4c296cbbe876",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Detected aspects</th>\n",
       "      <th>Corresponding sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>my family used my spectranet and they dont want to help my ministry now it has finished spectranetng abeg how will i change my password</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spectranetng how can i get the freedom mifi in ajah today</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>drolufunmilayo iconicremi spectranetng</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spectranetng your response just proves how horrid your customer service is rather than ask what my issue is to help resolve you render an apology apology accepted now can you actually proffer solution i am really disappointed with your services and attitude to customers</td>\n",
       "      <td>[customer service]</td>\n",
       "      <td>[Negative]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spectranet is just the worse tbh i cant even watch a 5min video without serious lagging</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>spectranet unlimited value for money</td>\n",
       "      <td>[price]</td>\n",
       "      <td>[Positive]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>from 30th may to date mtn mifi 10k spectranet 10250 smile 24000 mtn mobile data roughly 57k both mtn 14k and smile mifi14500 were bought within the last one month spectranet deceived me  isps have frustrated me</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>spectranetng fritzthejanitor will they help me attend to other issues as well</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>thefunkydee spectranetng im giving spectranetng a second thoughts with this im this thinking of switching from smile to spectranetng and this doesnt looks good what would you advise me to do</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>50gb gone in one week spectranetng na so i am just tired</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>377 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                             Tweets  \\\n",
       "0                                                                                                                                           my family used my spectranet and they dont want to help my ministry now it has finished spectranetng abeg how will i change my password   \n",
       "1                                                                                                                                                                                                                         spectranetng how can i get the freedom mifi in ajah today   \n",
       "2                                                                                                                                                                                                                                            drolufunmilayo iconicremi spectranetng   \n",
       "3    spectranetng your response just proves how horrid your customer service is rather than ask what my issue is to help resolve you render an apology apology accepted now can you actually proffer solution i am really disappointed with your services and attitude to customers   \n",
       "4                                                                                                                                                                                           spectranet is just the worse tbh i cant even watch a 5min video without serious lagging   \n",
       "..                                                                                                                                                                                                                                                                              ...   \n",
       "372                                                                                                                                                                                                                                            spectranet unlimited value for money   \n",
       "373                                                              from 30th may to date mtn mifi 10k spectranet 10250 smile 24000 mtn mobile data roughly 57k both mtn 14k and smile mifi14500 were bought within the last one month spectranet deceived me  isps have frustrated me   \n",
       "374                                                                                                                                                                                                   spectranetng fritzthejanitor will they help me attend to other issues as well   \n",
       "375                                                                                  thefunkydee spectranetng im giving spectranetng a second thoughts with this im this thinking of switching from smile to spectranetng and this doesnt looks good what would you advise me to do   \n",
       "376                                                                                                                                                                                                                        50gb gone in one week spectranetng na so i am just tired   \n",
       "\n",
       "       Detected aspects Corresponding sentiment  \n",
       "0                  None                    None  \n",
       "1                  None                    None  \n",
       "2                  None                    None  \n",
       "3    [customer service]              [Negative]  \n",
       "4                  None                    None  \n",
       "..                  ...                     ...  \n",
       "372             [price]              [Positive]  \n",
       "373                None                    None  \n",
       "374                None                    None  \n",
       "375                None                    None  \n",
       "376                None                    None  \n",
       "\n",
       "[377 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(absa_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f5e1f88-8c0f-410a-a48b-7ba70eac69a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'price': {'buy spectranetng',\n",
       "  'price',\n",
       "  'purchase',\n",
       "  'value',\n",
       "  'value spectranet'},\n",
       " 'speed': {'download speed',\n",
       "  'fast',\n",
       "  'internet speed',\n",
       "  'slow',\n",
       "  'slower',\n",
       "  'snail speed',\n",
       "  'speed',\n",
       "  'speed abeg',\n",
       "  'speeds'},\n",
       " 'reliability': {'network quality', 'reliable', 'usefulness'},\n",
       " 'coverage': {'coverage', 'insurance claim', 'network coverage'},\n",
       " 'customer service': {'business',\n",
       "  'company',\n",
       "  'customer',\n",
       "  'customer care',\n",
       "  'customer care line isnt',\n",
       "  'customer service',\n",
       "  'customer service experience',\n",
       "  'customers',\n",
       "  'disgusting customer service',\n",
       "  'ifes business',\n",
       "  'internet connection',\n",
       "  'internet service',\n",
       "  'internet service provider',\n",
       "  'isp business',\n",
       "  'network provider',\n",
       "  'network quality',\n",
       "  'network reception',\n",
       "  'network service',\n",
       "  'provider i',\n",
       "  'providers',\n",
       "  'reliable',\n",
       "  'service',\n",
       "  'service i',\n",
       "  'service provider',\n",
       "  'service subscription failure',\n",
       "  'services',\n",
       "  'services i',\n",
       "  'spectranet ltd internet subscription n18525',\n",
       "  'teleport service',\n",
       "  'ticket number internet',\n",
       "  'tizeti internet service provider'},\n",
       " 'trustworthiness': {'usefulness'}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aspects_with_implying_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86bf7687-1086-4c66-8523-0f1e96f07cc4",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.map_locations\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'pandas._libs.index.IndexEngine._call_map_locations'\n",
      "Traceback (most recent call last):\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5231, in pandas._libs.hashtable.PyObjectHashTable.map_locations\n",
      "TypeError: unhashable type: 'list'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Negative]                        40\n",
       "[Positive]                        18\n",
       "[Neutral]                         11\n",
       "[Negative, Negative]               5\n",
       "[Negative, Neutral]                1\n",
       "[Positive, Positive, Positive]     1\n",
       "Name: Corresponding sentiment, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "absa_df[absa_df['Detected aspects'].notnull()]['Corresponding sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2672b4ae-21f7-4496-94e3-ee766530deb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "absa_df.to_csv('../data/model-generated/tweet_absa_price_not_cost.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5250c73f-d6ab-4369-996b-d71f8093b639",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3212242e-c8bb-4eb7-8151-1284cad3a0a6",
   "metadata": {},
   "source": [
    "#### Load the true aspect and their sentiment predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "769b459f-ecae-4179-95c6-e9eeb212b607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Aspects</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>my family used my spectranet and they dont wan...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spectranetng how can i get the freedom mifi in...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>drolufunmilayo iconicremi spectranetng</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spectranetng your response just proves how hor...</td>\n",
       "      <td>['customer service']</td>\n",
       "      <td>['Negative']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spectranet is just the worse tbh i cant even w...</td>\n",
       "      <td>['speed']</td>\n",
       "      <td>['Negative']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tweets               Aspects  \\\n",
       "0  my family used my spectranet and they dont wan...                   NaN   \n",
       "1  spectranetng how can i get the freedom mifi in...                   NaN   \n",
       "2             drolufunmilayo iconicremi spectranetng                   NaN   \n",
       "3  spectranetng your response just proves how hor...  ['customer service']   \n",
       "4  spectranet is just the worse tbh i cant even w...             ['speed']   \n",
       "\n",
       "      Sentiment  \n",
       "0           NaN  \n",
       "1           NaN  \n",
       "2           NaN  \n",
       "3  ['Negative']  \n",
       "4  ['Negative']  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_aspects_df = pd.read_csv(\"../data/processed/absa_labelled_price_not_cost.csv\")\n",
    "true_aspects_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2f9149d-1a82-4290-8399-035bffb08fd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['customer service']\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_aspects_df.Aspects[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3dc919-a7bc-4861-a3c9-31a7b091a2e4",
   "metadata": {},
   "source": [
    "#### Reformat from string format to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "189ad9be-0fe7-468e-a9b3-2c3191dd39d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carry out conversion for Aspects column\n",
    "true_aspects_df.Aspects = true_aspects_df.Aspects.apply(lambda x: eval(x) if (pd.notnull(x)) else x)\n",
    "\n",
    "#Carry out conversion for Sentiment column\n",
    "true_aspects_df.Sentiment = true_aspects_df.Sentiment.apply(lambda x: eval(x) if (pd.notnull(x)) else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df085bd-c2a1-4b15-92b1-d734997433ae",
   "metadata": {},
   "source": [
    "#### Merge the true predictions and the model's predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91a68dbe-6ed1-4db4-95b9-9760ec5619fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Detected aspects</th>\n",
       "      <th>Corresponding sentiment</th>\n",
       "      <th>True Aspects</th>\n",
       "      <th>True Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>my family used my spectranet and they dont wan...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spectranetng how can i get the freedom mifi in...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>drolufunmilayo iconicremi spectranetng</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spectranetng your response just proves how hor...</td>\n",
       "      <td>[customer service]</td>\n",
       "      <td>[Negative]</td>\n",
       "      <td>[customer service]</td>\n",
       "      <td>[Negative]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spectranet is just the worse tbh i cant even w...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[speed]</td>\n",
       "      <td>[Negative]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tweets    Detected aspects  \\\n",
       "0  my family used my spectranet and they dont wan...                None   \n",
       "1  spectranetng how can i get the freedom mifi in...                None   \n",
       "2             drolufunmilayo iconicremi spectranetng                None   \n",
       "3  spectranetng your response just proves how hor...  [customer service]   \n",
       "4  spectranet is just the worse tbh i cant even w...                None   \n",
       "\n",
       "  Corresponding sentiment        True Aspects True Sentiment  \n",
       "0                    None                 NaN            NaN  \n",
       "1                    None                 NaN            NaN  \n",
       "2                    None                 NaN            NaN  \n",
       "3              [Negative]  [customer service]     [Negative]  \n",
       "4                    None             [speed]     [Negative]  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = absa_df.copy()\n",
    "merged_df[['True Aspects','True Sentiment']] = true_aspects_df[['Aspects','Sentiment']]\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f72a485-54d0-4b43-a0eb-3a4129ca3334",
   "metadata": {},
   "source": [
    "#### Fill Nones and NaNs with [None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b8338dd-2e72-4b36-9e01-373530942762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Detected aspects</th>\n",
       "      <th>Corresponding sentiment</th>\n",
       "      <th>True Aspects</th>\n",
       "      <th>True Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>my family used my spectranet and they dont wan...</td>\n",
       "      <td>[None]</td>\n",
       "      <td>[None]</td>\n",
       "      <td>[None]</td>\n",
       "      <td>[None]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spectranetng how can i get the freedom mifi in...</td>\n",
       "      <td>[None]</td>\n",
       "      <td>[None]</td>\n",
       "      <td>[None]</td>\n",
       "      <td>[None]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>drolufunmilayo iconicremi spectranetng</td>\n",
       "      <td>[None]</td>\n",
       "      <td>[None]</td>\n",
       "      <td>[None]</td>\n",
       "      <td>[None]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spectranetng your response just proves how hor...</td>\n",
       "      <td>[customer service]</td>\n",
       "      <td>[Negative]</td>\n",
       "      <td>[customer service]</td>\n",
       "      <td>[Negative]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spectranet is just the worse tbh i cant even w...</td>\n",
       "      <td>[None]</td>\n",
       "      <td>[None]</td>\n",
       "      <td>[speed]</td>\n",
       "      <td>[Negative]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tweets    Detected aspects  \\\n",
       "0  my family used my spectranet and they dont wan...              [None]   \n",
       "1  spectranetng how can i get the freedom mifi in...              [None]   \n",
       "2             drolufunmilayo iconicremi spectranetng              [None]   \n",
       "3  spectranetng your response just proves how hor...  [customer service]   \n",
       "4  spectranet is just the worse tbh i cant even w...              [None]   \n",
       "\n",
       "  Corresponding sentiment        True Aspects True Sentiment  \n",
       "0                  [None]              [None]         [None]  \n",
       "1                  [None]              [None]         [None]  \n",
       "2                  [None]              [None]         [None]  \n",
       "3              [Negative]  [customer service]     [Negative]  \n",
       "4                  [None]             [speed]     [Negative]  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = merged_df.apply(lambda s: s.fillna({i: [None] for i in df.index}))\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13a101f-7e66-441b-9424-6959c8ddcec3",
   "metadata": {},
   "source": [
    "### Aspect Extraction Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a9452f9-6f2c-4029-be8d-5a4bbe3f6bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def micro_precision_recall_f1_score(true_aspects,aspect_preds):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to compute the micro-averaged precision, recall and f1 score based on the model's predicitions\n",
    "    \n",
    "    Formulas guided by:\n",
    "    \n",
    "        - MICRO-PRECISION:\n",
    "          Micro-precision on the Peltarion Platform. (2021). Micro-precision on the Peltarion Platform.\n",
    "          Retrieved from https://peltarion.com/knowledge-center/documentation/evaluation-view/classification-loss-metrics/micro-precision\n",
    "        \n",
    "        - MICRO-RECALL:\n",
    "          Micro-recall on the Peltarion Platform. (2021). Micro-recall on the Peltarion Platform.\n",
    "          Retrieved from https://peltarion.com/knowledge-center/documentation/evaluation-view/classification-loss-metrics/micro-recall\n",
    "          \n",
    "        - MICRO-F1:\n",
    "          Micro F1-score on the Peltarion Platform. (2021). Micro F1-score on the Peltarion Platform. \n",
    "          Retrieved from https://peltarion.com/knowledge-center/documentation/evaluation-view/classification-loss-metrics/micro-f1-score\n",
    "  \n",
    "    Inputs:\n",
    "        - true_aspects (pandas Series): True aspects for each tweet\n",
    "        - aspect_preds (pandas Series): Model's predicted aspects for each tweet\n",
    "        \n",
    "    Outputs:\n",
    "        - class metrics (dict): Dictionary of class-level metrics: false positive, true positive and precision\n",
    "        - micro_precision (float): Micro-averaged precision\n",
    "        - micro_recall (float): Micro-averaged recall\n",
    "        - micro_f1 (float): Micro-averaged f1 score\n",
    "    \"\"\"\n",
    "    \n",
    "    #Dictionary to note the number of true positives, false positives and false negatives \n",
    "    #for the different classes\n",
    "    class_metrics = {}\n",
    "    \n",
    "    #Iterate through all the aspects\n",
    "    for aspect in aspects:\n",
    "        \n",
    "        #Initialize counters for true positives, false positives and false negatives\n",
    "        TP, FP, FN, TN = 0, 0, 0, 0\n",
    "        \n",
    "        #Iterate through all the tweets\n",
    "        for idx in range(len(aspect_preds)):\n",
    "            \n",
    "            #If the predicted aspect is truly in the tweet\n",
    "            if (aspect in aspect_preds[idx]) and (aspect in true_aspects[idx]):\n",
    "                \n",
    "                #Note a true positive\n",
    "                TP += 1\n",
    "            \n",
    "            #If the aspect is in the tweet but the model does not recognize it\n",
    "            if (aspect not in aspect_preds[idx]) and (aspect in true_aspects[idx]):\n",
    "                \n",
    "                #Note false negative\n",
    "                FN += 1\n",
    "                \n",
    "            #If the predicted aspect is not truly in the tweet\n",
    "            if (aspect in aspect_preds[idx]) and (aspect not in true_aspects[idx]):\n",
    "\n",
    "                #Record false positive\n",
    "                FP += 1\n",
    "                \n",
    "            #If the aspect was correctly left out of the tweet\n",
    "            if (aspect not in aspect_preds[idx]) and (aspect not in true_aspects[idx]):\n",
    "\n",
    "                #Record false positive\n",
    "                TN += 1\n",
    "        \n",
    "        #Calculate class level precision and recall\n",
    "        precision = float(TP/(TP+FP))\n",
    "        recall = float(TP/(TP+FN))\n",
    "        \n",
    "        #Calculate class level f1 score\n",
    "        try:\n",
    "            f1_score = 2*((precision*recall)/(precision+recall))\n",
    "        except ZeroDivisionError:\n",
    "            f1_score = 0\n",
    "\n",
    "        #Note down the final class-level metrics\n",
    "        class_metrics[aspect] = {'TP':TP, 'FP':FP, \n",
    "                                 'FN': FN, 'TN':TN,\n",
    "                                 'Precision': precision, \n",
    "                                 'Recall': recall,\n",
    "                                 'F1': f1_score}\n",
    "                \n",
    "        \n",
    "    #COMPUTE MICRO-AVERAGED PRECISION, RECALL AND F1\n",
    "    \n",
    "    #Counters to track class aggregated metrics\n",
    "    TP_sum, FP_sum, FN_sum = 0, 0, 0\n",
    "     \n",
    "    #Iterate through all the classes\n",
    "    for aspect_key in class_metrics.keys():\n",
    "        \n",
    "        #Get the TP\n",
    "        TP_sum += class_metrics[aspect_key]['TP']\n",
    "        \n",
    "        #Get the FP\n",
    "        FP_sum += class_metrics[aspect_key]['FP']\n",
    "        \n",
    "        #Get the FN\n",
    "        FN_sum += class_metrics[aspect_key]['FN']\n",
    "        \n",
    "    #Micro-precision\n",
    "    micro_precision = TP_sum/(TP_sum + FP_sum)\n",
    "    \n",
    "    #Micro recall\n",
    "    micro_recall = TP_sum/(TP_sum + FN_sum)\n",
    "    \n",
    "    #Micro F1\n",
    "    micro_f1 = 2*((micro_precision * micro_recall)/(micro_precision + micro_recall))\n",
    "    \n",
    "    #Return class level metrics, micro-precision, micro-recall and micro-f1\n",
    "    return class_metrics, micro_precision, micro_recall, micro_f1\n",
    "    \n",
    "#Run evaluation\n",
    "class_metrics, micro_precision, micro_recall, micro_f1 = micro_precision_recall_f1_score(merged_df['True Aspects'],merged_df['Detected aspects'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15723e50-67c1-424d-b5f9-13a58776fbcc",
   "metadata": {},
   "source": [
    "#### Class-level Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a015f4d3-b621-46be-8fde-3ac1e55e22ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>TN</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>price</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>349</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.352941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speed</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>343</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.612245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reliability</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>345</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coverage</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>359</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>customer service</th>\n",
       "      <td>20</td>\n",
       "      <td>33</td>\n",
       "      <td>5</td>\n",
       "      <td>319</td>\n",
       "      <td>0.377358</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.512821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trustworthiness</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>370</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  TP  FP  FN   TN  Precision    Recall        F1\n",
       "price              6   0  22  349   1.000000  0.214286  0.352941\n",
       "speed             15   1  18  343   0.937500  0.454545  0.612245\n",
       "reliability        4   0  28  345   1.000000  0.125000  0.222222\n",
       "coverage           3   1  14  359   0.750000  0.176471  0.285714\n",
       "customer service  20  33   5  319   0.377358  0.800000  0.512821\n",
       "trustworthiness    0   1   6  370   0.000000  0.000000  0.000000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(class_metrics).T.astype({'TP': 'int32','FP': 'int32','FN': 'int32','TN': 'int32'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b33ffea-31b0-449e-b82b-a3dfec9e248d",
   "metadata": {},
   "source": [
    "**DISCUSSION:**\n",
    "- Model does poorly retrieving all the different aspects\n",
    "- When the model does predict speed and reliability, it is correct most of the time. This does not hold up for the other aspects\n",
    "- As evidenced by the micro-averaged F1, the current model is not sufficient for our predictive task and could do with significant refinement or replacement altogether."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b19ef9-dc8b-4850-9213-933b1a78c6b6",
   "metadata": {},
   "source": [
    "#### Global-level Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d77bffe0-f7b1-4cb6-b60f-845d53c13e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-averaged precision: 0.571\n",
      "Micro-averaged recall: 0.34\n",
      "Micro-averaged F1 score: 0.427\n"
     ]
    }
   ],
   "source": [
    "print(f\"Micro-averaged precision: {round(micro_precision,3)}\")\n",
    "print(f\"Micro-averaged recall: {round(micro_recall,3)}\")\n",
    "print(f\"Micro-averaged F1 score: {round(micro_f1,3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab8b1c7-d712-4320-8f88-849838a9406f",
   "metadata": {},
   "source": [
    "There are certainly parameters that can be tweaked around/optimized (e.g. similarity score threshold, aspect category names, etc.) through a grid search, but I don't expect the improvement to be extremely significant, especially given my findings from the annotation phase (see section below). Even still, tinkering with these to improve performance doesn't make the model very robust"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399e0ca4-ad76-440a-b51b-f6140dd193e2",
   "metadata": {},
   "source": [
    "### Aspect Extraction Evaluation\n",
    "\n",
    "We only consider the sentiment predicted for the aspects the model determined correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "69f9e525-d47e-49a7-89df-2915fa81f508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aspect_sentiment_accuracy(true_aspects,aspect_preds,true_sentiment,sentiment_preds):\n",
    "    \n",
    "    #Dictionary to note the sentiment prediction accuracy for the different aspects\n",
    "    aspect_accuracy = {}\n",
    "    \n",
    "    #Track the number of correct preds and total preds across all the classes\n",
    "    global_correct_preds, global_total_preds = 0, 0\n",
    "    \n",
    "    #Iterate through all the aspects\n",
    "    for aspect in aspects:\n",
    "        \n",
    "        #Initialize counters for number of correct predictions and number of total predictions\n",
    "        correct_preds, total_preds = 0, 0\n",
    "        \n",
    "        #Iterate through all the tweets\n",
    "        for idx in range(len(aspect_preds)):\n",
    "            \n",
    "            #If the predicted aspect is truly in the tweet\n",
    "            if (aspect in aspect_preds[idx]) and (aspect in true_aspects[idx]):\n",
    "                \n",
    "                #Format the predicted list to a numpy array\n",
    "                model_preds = np.array(aspect_preds[idx]) #Model preds\n",
    "                \n",
    "                true_preds = np.array(true_aspects[idx]) #True preds\n",
    "                \n",
    "                #Find the corresponding sentiment of the correctly predicted aspect\n",
    "                #1. In model preds\n",
    "                sentiment_pred = sentiment_preds[idx][np.argwhere(model_preds == aspect)[0][0]]\n",
    "                \n",
    "                #1. In true preds\n",
    "                true_sentiment_pred = true_sentiment[idx][np.argwhere(true_preds == aspect)[0][0]]\n",
    "                \n",
    "                #If the predicted sentiment for the aspect is equal to the true sentiment\n",
    "                if sentiment_pred == true_sentiment_pred:\n",
    "                    \n",
    "                    #Record as correct prediction\n",
    "                    correct_preds += 1\n",
    "                    global_correct_preds += 1 #Global case\n",
    "                    \n",
    "                \n",
    "                #Record a prediction regardless of if correct or not\n",
    "                total_preds += 1\n",
    "                global_total_preds += 1 #Global case\n",
    "                \n",
    "                \n",
    "        #Note down the final class-level accuracy\n",
    "        try:\n",
    "            aspect_accuracy[aspect] = correct_preds/total_preds\n",
    "        except ZeroDivisionError:\n",
    "            aspect_accuracy[aspect] = 'No correct aspect detection for this aspect'\n",
    "            \n",
    "                \n",
    "    #Compute the global/micro accuracy (across all aspects)\n",
    "    micro_accuracy = global_correct_preds/global_total_preds\n",
    "    \n",
    "    #Compute the macro accuracy (unweighted average from all the classes)\n",
    "    macro_accuracy = np.mean([aspect_accuracy[aspect] for aspect in aspect_accuracy.keys() if isinstance(aspect_accuracy[aspect],float)])\n",
    "    \n",
    "    #Return class level metrics, micro-accuracy, and macro accuracy\n",
    "    return aspect_accuracy, micro_accuracy, macro_accuracy\n",
    "    \n",
    "#Run evaluation\n",
    "class_metrics, micro_accuracy, macro_accuracy = aspect_sentiment_accuracy(merged_df['True Aspects'],\n",
    "                                                                          merged_df['Detected aspects'],\n",
    "                                                                          merged_df['True Sentiment'],\n",
    "                                                                          merged_df['Corresponding sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "43c3d153-53f0-4e2f-a78b-ba3dd559d919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'price': 0.8333333333333334,\n",
       " 'speed': 0.7333333333333333,\n",
       " 'reliability': 1.0,\n",
       " 'coverage': 0.6666666666666666,\n",
       " 'customer service': 0.8,\n",
       " 'trustworthiness': 'No correct aspect detection for this aspect'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "15c63ea2-3b43-4eaa-b27a-f7a7a9d62142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7916666666666666"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "micro_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "460c1ea5-9639-4b4c-a806-626617fbf49e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8066666666666666"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macro_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a63323-a6d3-45ec-86d4-715dfadf5a2c",
   "metadata": {},
   "source": [
    "The model does pretty well determining the sentiment when the aspect is actually correctly determined. We see micro and macro accuracy scores at about 80%. We note however that the model performs poorly predicting the sentiment for coverage. It predicts perfectly for reliability.\n",
    "\n",
    "Also we note that after switching 'cost' in the previous implementation with 'price,' the model does much better on both extraction and sentiment category prediction\n",
    "\n",
    "**Note:** This part of the model was implemented using the ABSA package by ScalaConsultants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca045ca-51dc-4c31-abc0-5a1936b26db0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e83eed7-7042-43c8-8e39-06f5adfd9165",
   "metadata": {},
   "source": [
    "The above results obtained using the ABSA package by ScalaConsultants were obtained by passing in words that were specifically in the tweets. Based on the documentation, the ABSA package is also able to determine sentiment towards an aspect even when the aspect is not explicitly mentioned. This is explored below. Depending on the results, we might not need to focus efforts on creating an entirely new aspect category sentiment prediction pipeline. We can instead simply focus efforts on improving aspect category extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a56aba-6ae1-4ebc-af99-6017b29de7a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2a29498-1332-4bad-99ab-9588a33529b1",
   "metadata": {},
   "source": [
    "### ABSA using aspect categories rather than aspect terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "efe9b9ae-e07e-4029-b381-a98b3a6b3986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Aspects</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Detected aspects</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spectranetng your response just proves how hor...</td>\n",
       "      <td>[customer service]</td>\n",
       "      <td>[Negative]</td>\n",
       "      <td>[customer service]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spectranetng dontusespectranet incredibly poor...</td>\n",
       "      <td>[coverage, customer service, price]</td>\n",
       "      <td>[Negative, Negative, Negative]</td>\n",
       "      <td>[coverage, customer service]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spectranetng its a disgusting customer service...</td>\n",
       "      <td>[customer service]</td>\n",
       "      <td>[Negative]</td>\n",
       "      <td>[customer service]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spectranetng hwfa why so slow</td>\n",
       "      <td>[speed]</td>\n",
       "      <td>[Negative]</td>\n",
       "      <td>[speed]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kunleeko drdammie giditraffic plentygadgets dr...</td>\n",
       "      <td>[speed]</td>\n",
       "      <td>[Negative]</td>\n",
       "      <td>[customer service]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tweets  \\\n",
       "0  spectranetng your response just proves how hor...   \n",
       "1  spectranetng dontusespectranet incredibly poor...   \n",
       "2  spectranetng its a disgusting customer service...   \n",
       "3                      spectranetng hwfa why so slow   \n",
       "4  kunleeko drdammie giditraffic plentygadgets dr...   \n",
       "\n",
       "                               Aspects                       Sentiment  \\\n",
       "0                   [customer service]                      [Negative]   \n",
       "1  [coverage, customer service, price]  [Negative, Negative, Negative]   \n",
       "2                   [customer service]                      [Negative]   \n",
       "3                              [speed]                      [Negative]   \n",
       "4                              [speed]                      [Negative]   \n",
       "\n",
       "               Detected aspects  \n",
       "0            [customer service]  \n",
       "1  [coverage, customer service]  \n",
       "2            [customer service]  \n",
       "3                       [speed]  \n",
       "4            [customer service]  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Drop NAs in true aspect dataframe\n",
    "tdf = true_aspects_df.dropna(subset=['Aspects','Sentiment']).reset_index(drop=True)\n",
    "\n",
    "#Merge both dataframes (true dataframe and preds dataframe) on the tweet text\n",
    "tdf_and_preds = pd.merge(tdf,absa_df.dropna()[['Tweets','Detected aspects']],on='Tweets',how='inner')\n",
    "\n",
    "#Quick preview\n",
    "tdf_and_preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "488cc93a-bf5c-490c-a928-f4b465e6a4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sentiment_with_categories(tweets,detected_aspects):\n",
    "    \n",
    "    #List to store the tweet aspect sentiments\n",
    "    tweet_aspect_sentiments = []\n",
    "    \n",
    "    for idx in range(len(detected_aspects)):\n",
    "        \n",
    "        #Calculate the sentiment for the different detected aspects\n",
    "        sentiment = nlp(tweets[idx] ,aspects = detected_aspects[idx])\n",
    "        \n",
    "        #Add the predicted sentiment to the list\n",
    "        tweet_aspect_sentiments.append([str(sentiment.subtasks[aspect].sentiment).replace('Sentiment.','').capitalize() for aspect in sentiment.aspects])\n",
    "        \n",
    "    \n",
    "    df = pd.concat([tweets, detected_aspects], axis=1).reset_index(drop=True)\n",
    "    df['Corresponding sentiment'] = tweet_aspect_sentiments\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b0563f63-f5da-4a67-9247-a10346684434",
   "metadata": {},
   "outputs": [],
   "source": [
    "absa_cat = compute_sentiment_with_categories(tdf_and_preds.Tweets,\n",
    "                                             tdf_and_preds['Detected aspects'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "39a1a423-7ad8-4bf7-83c3-ced94372e560",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_metrics, micro_accuracy, macro_accuracy = aspect_sentiment_accuracy(tdf_and_preds['Aspects'],\n",
    "                                                                          absa_cat['Detected aspects'],\n",
    "                                                                          tdf_and_preds['Sentiment'],\n",
    "                                                                          absa_cat['Corresponding sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c0beb197-7a81-4e36-b611-3a74ad414480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'price': 0.6666666666666666,\n",
       " 'speed': 0.8,\n",
       " 'reliability': 1.0,\n",
       " 'coverage': 0.6666666666666666,\n",
       " 'customer service': 0.8947368421052632,\n",
       " 'trustworthiness': 'No correct aspect detection for this aspect'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0bfb9a17-75ff-4cd1-8a23-1d2361be576f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8260869565217391"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "micro_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "10878274-2a91-4a30-8e8e-c7871b3a1d08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8056140350877193"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macro_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115b6830-925a-491e-ba8c-5f9887f9fd92",
   "metadata": {},
   "source": [
    "ABSA model does even better when just the aspect categories are fed in! Not the aspect terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795d3b28-77cb-4bd3-a301-7b89216b5112",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c48709f-6672-4af2-a606-8cdcd7fa82b8",
   "metadata": {},
   "source": [
    "# Annotating the 2nd ABSA data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "11fff484-77e6-4c95-ba11-150d21a65b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/koredeakande/opt/anaconda3/envs/capstone/lib/python3.7/site-packages/ipykernel_launcher.py:111: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n"
     ]
    }
   ],
   "source": [
    "#Set to store all seen words\n",
    "seen_words = set()\n",
    "\n",
    "#Set to store all aspect implying words found – to avoid recomputing similarity scores\n",
    "aspect_implying_words_glob = set()\n",
    "\n",
    "#Dictionary categorizing all aspect-implying words into their relevant aspects\n",
    "aspects_with_implying_words = {'price':set(),'speed':set(),'reliability':set(),'coverage':set(), \n",
    "                               'customer service':set(),'trustworthiness':set()}\n",
    "\n",
    "#List to store detected aspects and their sentiments\n",
    "df_list = []\n",
    "\n",
    "#Similarity threshold\n",
    "sim_thresh = 0.6\n",
    "\n",
    "#Chunk tags to match – i.e. parts of speech to extract\n",
    "CHUNK_TAG = \"\"\"\n",
    "MATCH: {<NN>+|<NN.*>+}\n",
    "{<JJ.*>?}\n",
    "{<RB.*>?}\n",
    "\"\"\"\n",
    "\n",
    "#Initialize chunk tag parser\n",
    "cp = nltk.RegexpParser(CHUNK_TAG)\n",
    "\n",
    "#Iterate through all the tweets\n",
    "for tweet in new_annotations['Cleaned text']:\n",
    "    \n",
    "    #Set to store the detected aspects at the sentence level\n",
    "    # detected_aspects = set()\n",
    "    \n",
    "    #Dictionary to store the sentiment value for each seen aspect\n",
    "    sentence_lvl_aspect_sentiment = {'price':[],'speed':[],'reliability':[],'coverage':[], \n",
    "                                     'customer service':[], 'trustworthiness':[]}\n",
    "        \n",
    "    #Split the tweet into words\n",
    "    text = tweet.split()\n",
    "\n",
    "    #Tag the words with their part of speech\n",
    "    tokens_tag = pos_tag(text)\n",
    "    \n",
    "    #Get the words with relevant POS (noun, adverbs, adjectives)\n",
    "    chunk_result = cp.parse(tokens_tag)\n",
    "    \n",
    "    #Extract chunk results from tree into list \n",
    "    chunk_items = [list(n) for n in chunk_result if isinstance(n, nltk.tree.Tree)]\n",
    "    \n",
    "    #Finally fuse/extract chunked words to get (noun) phrases, nouns, adverbs, adjectives\n",
    "    #1. List to store the words\n",
    "    matched_words = []\n",
    "    \n",
    "    #2. Iterate through the chunked words lists and get the relevant words\n",
    "    for item in chunk_items:\n",
    "        if len(item) > 1:\n",
    "            full_string = []\n",
    "\n",
    "            for word in item:\n",
    "                full_string.append(word[0])\n",
    "\n",
    "            matched_words.append(' '.join(full_string))\n",
    "\n",
    "        else:\n",
    "            matched_words.append(item[0][0])\n",
    "        \n",
    "    #Iterate through all the words\n",
    "    for word_in_focus in matched_words:\n",
    "        \n",
    "        #If the word has been seen before\n",
    "        if word_in_focus in seen_words:\n",
    "            \n",
    "            #Check if the word is an aspect-implying word\n",
    "            if word_in_focus in aspect_implying_words_glob:\n",
    "                \n",
    "                #List to store all the aspects found to related to the certain word/token\n",
    "                aspects_implied = []\n",
    "            \n",
    "                #If it is an aspect-implying word, iterate through all the aspects\n",
    "                for aspect in aspects_with_implying_words.keys():\n",
    "                    \n",
    "                    #Check if the word_in_focus was noted as a word implying the aspect\n",
    "                    if word_in_focus in aspects_with_implying_words[aspect]:\n",
    "                        \n",
    "                        #Get all the aspects the word_in_focus implies\n",
    "                        aspects_implied.append(aspect)\n",
    "                        \n",
    "            \n",
    "            else:\n",
    "                continue\n",
    "                    \n",
    "         \n",
    "        #If the word hasn't been seen before\n",
    "        else:\n",
    "            \n",
    "            #Mark the word as seen now\n",
    "            seen_words.add(word_in_focus)\n",
    "                \n",
    "            #List to store all the aspects found to related to the certain word/token\n",
    "            #Ideally a given word won't imply multiple of the aspects as they are fairly independent\n",
    "            #-but just in case \n",
    "            aspects_implied = []\n",
    "\n",
    "            #Iterate through all the aspects\n",
    "            for aspect,asp_token in aspects_with_token:\n",
    "\n",
    "                #Translate word_in_focus to word vector\n",
    "                spacy_token = spacy_nlp(word_in_focus)\n",
    "\n",
    "                #Compute the similarity between the two word vectors (i.e. the two words)\n",
    "                #Round up to 1 d.p.\n",
    "                similarity_score = round(asp_token.similarity(spacy_token),1)\n",
    "\n",
    "                #If the max similarity score seen is greater than the threshold\n",
    "                if similarity_score >= sim_thresh:\n",
    "\n",
    "                    #Add the word to the set of all aspect-implying words seen\n",
    "                    aspect_implying_words_glob.add(word_in_focus)\n",
    "\n",
    "                    #Add the word to the dictionary of the relevant aspect word\n",
    "                    aspects_with_implying_words[aspect].add(word_in_focus)\n",
    "\n",
    "                    #Note that the aspect has been found in this particular sentence\n",
    "                    # detected_aspects.add(aspect)\n",
    "\n",
    "                    #Add the aspect to the list of aspects that the word_in_focus implies\n",
    "                    aspects_implied.append(aspect)\n",
    "\n",
    "\n",
    "                #If the word is not an aspect implying word, continue to next word\n",
    "                else:\n",
    "\n",
    "                    continue\n",
    "                \n",
    "        #Calculate the sentiment scores for the aspect_implying word in the current sentence\n",
    "        sentiment = nlp(tweet ,aspects = [word_in_focus])\n",
    "        sentiment_scores = sentiment.subtasks[word_in_focus].examples[0].scores\n",
    "\n",
    "        #Note down the scores for all the implied aspects\n",
    "        for aspect in aspects_implied:\n",
    "            sentence_lvl_aspect_sentiment[aspect].append(sentiment_scores)\n",
    "    \n",
    "    #List to store the detected aspects from the sentence\n",
    "    detected_aspects = []\n",
    "    \n",
    "    #List to store the determined sentiments of the detected aspects\n",
    "    detected_sentiments = []\n",
    "    \n",
    "    #Iterate through all the aspects\n",
    "    for aspect in sentence_lvl_aspect_sentiment.keys():\n",
    "        \n",
    "        #If the aspect was detected in the sentence\n",
    "        if sentence_lvl_aspect_sentiment[aspect]:\n",
    "            \n",
    "            #Record this\n",
    "            detected_aspects.append(aspect)\n",
    "            \n",
    "            #Calculate the average sentiment scores across the different terms\n",
    "            avg_senti_score = np.array(sentence_lvl_aspect_sentiment[aspect]).mean(axis=0)\n",
    "            \n",
    "            #Get the sentiment category (neutral,negative,positive) with the largest probability\n",
    "            max_idx = np.argmax(avg_senti_score)\n",
    "\n",
    "            if max_idx == 2:\n",
    "\n",
    "                detected_sentiments.append(\"Positive\")\n",
    "\n",
    "            elif max_idx == 1:\n",
    "\n",
    "                detected_sentiments.append(\"Negative\")\n",
    "\n",
    "            else:\n",
    "\n",
    "                detected_sentiments.append(\"Neutral\")\n",
    "    \n",
    "    #Add the detected aspects and sentiments from the sentence to the list\n",
    "    if detected_aspects:\n",
    "        df_list.append([tweet,detected_aspects,detected_sentiments])\n",
    "    else:\n",
    "        df_list.append([tweet,None,None])\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d886c0-d133-4ed9-8f9f-12eea262a764",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffa46c4-b903-4af8-95ce-5a84e486a277",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "91b4fff3-5a1c-4a42-a625-5ee5d96ed18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "newly_annotated_df = pd.DataFrame(df_list, \n",
    "                                  columns=['Cleaned tweets','Detected aspects','Corresponding sentiment'])\n",
    "newly_annotated_df.insert(0,'Tweets',new_annotations.Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cdfbff64-6718-433c-9749-c9f9b462c004",
   "metadata": {},
   "outputs": [],
   "source": [
    "newly_annotated_df.to_csv('../data/model-generated/tweet_absa_second_annotation.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
