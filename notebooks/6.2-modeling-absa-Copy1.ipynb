{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fba88215-aab4-4b5a-b8f5-80fc0d628f93",
   "metadata": {},
   "source": [
    "# Modeling: Aspect-Based Sentiment Analysis [Simplistic]\n",
    "Conducting aspect-based sentiment analysis with [ABSA package by Scala Consultants](https://github.com/ScalaConsultants/Aspect-Based-Sentiment-Analysis)\n",
    "\n",
    "**`Goal:`** \n",
    "\n",
    "Conduct ABSA using word similarity and out-of-the-box ABSA package. This notebook is meant to serve as a start for tweet aspect annotation by getting as much of the aspects indicated and their corresponding sentiments. \n",
    "\n",
    "**Note:** Results will be crosschecked during the annotation phase!\n",
    "\n",
    "**`Process:`** \n",
    "1. List aspects (e.g. speed, price, reliability) determined from earlier data annotation phase\n",
    "2. Get nouns, adjectives and adverbs from the tweets as these will likely be the parts of speech making meaningful reference to aspects\n",
    "3. Check if each of the words from step 2 is very similar to any of the aspects (e.g. speed [aspect] and fast [word in tweet]) by computing similarity score\n",
    "4. If similarity score is past a set thresholdhood, we assume the aspect was referenced in the tweet. Hence, note down that the aspect was referenced in that given tweet and also note down the word (herein called aspect-implying word) that implied the aspect\n",
    "6. Conduct ABSA using the ABSA package with the tweet and with the aspect-implying word and note sentiment (positive, negative or neutral) towards the main aspect (price, speed, etc.)\n",
    "7. If multiple words make reference to a single aspect, find the average of their sentiments and use to assign a single sentiment "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c11b87e-67cd-44e7-a51c-e26a44c442f4",
   "metadata": {},
   "source": [
    "### 1. Library Importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6fb6b147-1485-4aa1-b037-dd4a29d7fd84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting aspect_based_sentiment_analysis==2.0.2\n",
      "  Using cached aspect_based_sentiment_analysis-2.0.2-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: pytest in /Users/koredeakande/opt/anaconda3/envs/jupyterlab/lib/python3.9/site-packages (from aspect_based_sentiment_analysis==2.0.2) (6.2.5)\n",
      "Collecting transformers==2.5\n",
      "  Downloading transformers-2.5.0-py3-none-any.whl (481 kB)\n",
      "\u001b[K     |████████████████████████████████| 481 kB 2.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn in /Users/koredeakande/opt/anaconda3/envs/jupyterlab/lib/python3.9/site-packages (from aspect_based_sentiment_analysis==2.0.2) (1.0.1)\n",
      "Requirement already satisfied: testfixtures in /Users/koredeakande/opt/anaconda3/envs/jupyterlab/lib/python3.9/site-packages (from aspect_based_sentiment_analysis==2.0.2) (6.18.3)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==2.2 (from aspect-based-sentiment-analysis) (from versions: 2.5.0rc0, 2.5.0rc1, 2.5.0rc2, 2.5.0rc3, 2.5.0, 2.5.1, 2.6.0rc0, 2.6.0rc1, 2.6.0rc2, 2.6.0, 2.7.0rc0, 2.7.0rc1)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for tensorflow==2.2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install aspect_based_sentiment_analysis==2.0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "44ae5bf6-f5f7-4d2d-8f44-254749429d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: transformers\n",
      "Version: 4.11.3\n",
      "Summary: State-of-the-art Natural Language Processing for TensorFlow 2.0 and PyTorch\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Sam Shleifer, Patrick von Platen, Sylvain Gugger, Suraj Patil, Stas Bekman, Google AI Language Team Authors, Open AI team Authors, Facebook AI Authors, Carnegie Mellon University Authors\n",
      "Author-email: thomas@huggingface.co\n",
      "License: Apache\n",
      "Location: /Users/koredeakande/opt/anaconda3/envs/jupyterlab/lib/python3.9/site-packages\n",
      "Requires: filelock, requests, huggingface-hub, sacremoses, tokenizers, packaging, regex, pyyaml, tqdm, numpy\n",
      "Required-by: aspect-based-sentiment-analysis\n"
     ]
    }
   ],
   "source": [
    "!pip show transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7ef903c6-788c-4790-ae9a-8e7eaf9dac83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import aspect_based_sentiment_analysis as absa\n",
    "import nltk\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import wordnet as wn\n",
    "from itertools import product\n",
    "from nltk import pos_tag, RegexpParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3ecd53a8-f967-4a9a-bc60-6f4ed42bd8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: aspect-based-sentiment-analysis\n",
      "Version: 2.0.0\n",
      "Summary: Aspect Based Sentiment Analysis: Transformer & Interpretability (TensorFlow)\n",
      "Home-page: https://github.com/ScalaConsultants/Aspect-Based-Sentiment-Analysis\n",
      "Author: Rafal Rolczynski\n",
      "Author-email: rafal.rolczynski@gmail.com\n",
      "License: Apache-2.0\n",
      "Location: /Users/koredeakande/opt/anaconda3/envs/jupyterlab/lib/python3.9/site-packages\n",
      "Requires: google-cloud-storage, pytest, ipython, spacy, scikit-learn, testfixtures, optuna, transformers, tensorflow\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show aspect_based_sentiment_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f196af6-46fc-46fc-b124-d0c0f8a490e6",
   "metadata": {},
   "source": [
    "### 2. Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d50c70f7-0099-4f15-8e56-74c646ee921f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/interim/sample_with_sentiment_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "69980992-6dfc-4133-b932-02487abb7c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: transformers\n",
      "Version: 4.11.3\n",
      "Summary: State-of-the-art Natural Language Processing for TensorFlow 2.0 and PyTorch\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Sam Shleifer, Patrick von Platen, Sylvain Gugger, Suraj Patil, Stas Bekman, Google AI Language Team Authors, Open AI team Authors, Facebook AI Authors, Carnegie Mellon University Authors\n",
      "Author-email: thomas@huggingface.co\n",
      "License: Apache\n",
      "Location: /Users/koredeakande/opt/anaconda3/envs/jupyterlab/lib/python3.9/site-packages\n",
      "Requires: filelock, regex, tqdm, sacremoses, packaging, huggingface-hub, pyyaml, tokenizers, numpy, requests\n",
      "Required-by: aspect-based-sentiment-analysis\n"
     ]
    }
   ],
   "source": [
    "!pip show transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cf544a1f-7cb7-416e-ada7-ea1b01f65f9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46c71403bfc241ed8c3d0932eb205761",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.06k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40bb598d167b4381ae6671368642f400",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/418M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "The first argument to `Layer.call` must always be passed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/j5/540q0bw12gx3g56qg4llbmlh0000gn/T/ipykernel_10757/3189165022.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'absa/classifier-lapt-0.2'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabsa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBertABSClassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprofessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabsa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProfessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# Explained in detail later on.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtext_splitter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabsa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentencizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# The English CNN model from SpaCy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/jupyterlab/lib/python3.9/site-packages/transformers/modeling_tf_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   1436\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdummy_inputs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# build the network with dummy inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1437\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1438\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdummy_inputs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# build the network with dummy inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolved_archive_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Error retrieving file {resolved_archive_file}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/jupyterlab/lib/python3.9/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1036\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1037\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/jupyterlab/lib/python3.9/site-packages/aspect_based_sentiment_analysis/models.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, token_ids, attention_mask, token_type_ids, training, **bert_kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mbert_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     ) -> Tuple[tf.Tensor, Tuple[tf.Tensor, ...], Tuple[tf.Tensor, ...]]:\n\u001b[0;32m--> 141\u001b[0;31m         outputs = self.bert(\n\u001b[0m\u001b[1;32m    142\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/jupyterlab/lib/python3.9/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    965\u001b[0m     \u001b[0;31m# - mixed precision casting (autocast) is only applied to `inputs`,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m     \u001b[0;31m#   not to any other argument.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 967\u001b[0;31m     \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_split_out_first_arg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    968\u001b[0m     \u001b[0minput_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/jupyterlab/lib/python3.9/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_split_out_first_arg\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3009\u001b[0m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fn_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3010\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3011\u001b[0;31m       raise ValueError(\n\u001b[0m\u001b[1;32m   3012\u001b[0m           'The first argument to `Layer.call` must always be passed.')\n\u001b[1;32m   3013\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The first argument to `Layer.call` must always be passed."
     ]
    }
   ],
   "source": [
    "name = 'absa/classifier-lapt-0.2'\n",
    "model = absa.BertABSClassifier.from_pretrained(name)\n",
    "tokenizer = BertTokenizer.from_pretrained(name)\n",
    "professor = absa.Professor()     # Explained in detail later on.\n",
    "text_splitter = absa.sentencizer()  # The English CNN model from SpaCy.\n",
    "nlp = absa.Pipeline(model, tokenizer, professor, text_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3c975f1e-8741-4a3c-917f-85b9238e0e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mabsa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'absa/classifier-rest-0.2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtext_splitter\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreference_recognizer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0maspect_based_sentiment_analysis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maux_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReferenceRecognizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpattern_recognizer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0maspect_based_sentiment_analysis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maux_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPatternRecognizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0maspect_based_sentiment_analysis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipelines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Load ready to use pipelines. Files are stored on\n",
       "the HaggingFace AWS S3. \n",
       "\u001b[0;31mFile:\u001b[0m      ~/opt/anaconda3/envs/jupyterlab/lib/python3.9/site-packages/aspect_based_sentiment_analysis/loads.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?absa.load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3b8d956e-53a3-4dc9-8da3-dfa6b60d3c9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39dba5bfa3a24f0ca755637fb5592cd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.01G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "The first argument to `Layer.call` must always be passed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/j5/540q0bw12gx3g56qg4llbmlh0000gn/T/ipykernel_10757/3456758127.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Load the basic configuration of the ABSA package\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabsa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert-base-multilingual-cased'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/jupyterlab/lib/python3.9/site-packages/aspect_based_sentiment_analysis/loads.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, text_splitter, reference_recognizer, pattern_recognizer, **model_kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertABSCConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertABSClassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mprofessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProfessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreference_recognizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpattern_recognizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/jupyterlab/lib/python3.9/site-packages/transformers/modeling_tf_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   1436\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdummy_inputs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# build the network with dummy inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1437\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1438\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdummy_inputs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# build the network with dummy inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolved_archive_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Error retrieving file {resolved_archive_file}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/jupyterlab/lib/python3.9/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1036\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1037\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/jupyterlab/lib/python3.9/site-packages/aspect_based_sentiment_analysis/models.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, token_ids, attention_mask, token_type_ids, training, **bert_kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mbert_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     ) -> Tuple[tf.Tensor, Tuple[tf.Tensor, ...], Tuple[tf.Tensor, ...]]:\n\u001b[0;32m--> 141\u001b[0;31m         outputs = self.bert(\n\u001b[0m\u001b[1;32m    142\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/jupyterlab/lib/python3.9/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    965\u001b[0m     \u001b[0;31m# - mixed precision casting (autocast) is only applied to `inputs`,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m     \u001b[0;31m#   not to any other argument.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 967\u001b[0;31m     \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_split_out_first_arg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    968\u001b[0m     \u001b[0minput_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/jupyterlab/lib/python3.9/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_split_out_first_arg\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3009\u001b[0m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fn_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3010\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3011\u001b[0;31m       raise ValueError(\n\u001b[0m\u001b[1;32m   3012\u001b[0m           'The first argument to `Layer.call` must always be passed.')\n\u001b[1;32m   3013\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The first argument to `Layer.call` must always be passed."
     ]
    }
   ],
   "source": [
    "#Load the basic configuration of the ABSA package\n",
    "nlp = absa.load('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5372bfb5-132c-41d5-9b84-05ae1dfa8974",
   "metadata": {},
   "outputs": [],
   "source": [
    "#List aspects determined during the annotation phase\n",
    "#Note: This might not be exhaustive! But it should cover most cases. It is also subjective!\n",
    "#Also using synonyms of these words will likely yield different results\n",
    "aspects = ['price','speed','reliability','coverage', 'customer service']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "159d93a0-4570-41c9-9df1-663573e5f1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set to store all seen words\n",
    "seen_words = set()\n",
    "\n",
    "#Set to store all aspect implying words found to avoid recomputing similarity words\n",
    "aspect_implying_words_glob = set()\n",
    "\n",
    "aspects_with_implying_words = {'price':set(),'speed':set(),'reliability':set(),\n",
    "                               'coverage':set(), 'customer service':set()}\n",
    "\n",
    "#Similarity threshold\n",
    "sim_thresh = 0.4\n",
    "\n",
    "#Iterate through all the tweets\n",
    "for tweet in df.Text:\n",
    "        \n",
    "    #Split the tweet into words\n",
    "    text = tweet.split()\n",
    "\n",
    "    #Tag words with part of speech\n",
    "    tokens_tag = pos_tag(text)\n",
    "\n",
    "    #Iterate through all the tagged words\n",
    "    for token in tokens_tag:\n",
    "        \n",
    "        #Check if the tagged word is a noun, adjective or adverb\n",
    "        regex_match = re.match('NN.?|JJ.?|RB.?',token[1])\n",
    "\n",
    "        #If it is one of the mentioned parts of speech\n",
    "        if regex_match:\n",
    "            \n",
    "            #Get the word\n",
    "            word_in_focus = token[0]\n",
    "        \n",
    "            #If the word has not been before\n",
    "            if word_in_focus not in seen_words:\n",
    "            \n",
    "                #Iterate through all the aspects and compute similarity/relatedness\n",
    "                for aspect in aspects:\n",
    "\n",
    "                    #Look up the words on wordnet – this gets multiple versions of the word\n",
    "                    sem1, sem2 = wn.synsets(aspect), wn.synsets(word_in_focus)\n",
    "\n",
    "                    #Iterate through different permutations of the versions of the words\n",
    "                    #and get the max similarity score seen\n",
    "                    maxscore = 0\n",
    "\n",
    "                    for i,j in list(product(*[sem1,sem2])):\n",
    "                      score = i.wup_similarity(j) # Wu-Palmer Similarity\n",
    "                      maxscore = score if maxscore < score else maxscore\n",
    "\n",
    "                    #If the max similarity score seen is greater than the threshold\n",
    "                    if maxscore > sim_thresh:\n",
    "\n",
    "                        #Add the word to the set of all aspect-implying words seen\n",
    "                        aspect_implying_words_glob.add(word_in_focus)\n",
    "\n",
    "                        #Add the word to the dictionary of the relevant aspect word\n",
    "                        aspects_with_implying_words[aspect].add(word_in_focus)\n",
    "                        \n",
    "                seen_words.add(word_in_focus)\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                    \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "        \n",
    "text =\"learn php from guru99 and make study easy\".split()\n",
    "print(\"After Split:\",text)\n",
    "tokens_tag = pos_tag(text)\n",
    "print(\"After Token:\",tokens_tag)\n",
    "patterns= \"\"\"mychunk:{<NN.?>*<VBD.?>*<JJ.?>*<CC>?}\"\"\"\n",
    "chunker = RegexpParser(patterns)\n",
    "print(\"After Regex:\",chunker)\n",
    "output = chunker.parse(tokens_tag)\n",
    "print(\"After Chunking\",output)\n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7a9f44b6-73e6-4b4b-bda0-b521847f482b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'k' in set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c96770a-f1d4-4e6c-8c92-d6f9abfc9e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ac4da9-0ce1-460c-9e3a-535d00331d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in tokens_tag:\n",
    "    \n",
    "    if token[1] in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "de0991a7-b2f0-4ffc-8b17-76753a2b8224",
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_match = re.match('NN.?|JJ.?|RB.?','LPP')\n",
    "            \n",
    "if regex_match:\n",
    "    \n",
    "    print(regex_match[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eed5afcc-75d3-47d7-9616-3e9ed9f761ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = re.match('NN.?|JJ.?|RB.?','L')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c0f38a8a-8a3a-41a5-bb27-1b3b5675f232",
   "metadata": {},
   "outputs": [],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2235d31a-66e9-4f50-8755-1873ba67ede1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Try to apply the pattern at the start of the string, returning\n",
       "a Match object, or None if no match was found.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/opt/anaconda3/envs/jupyterlab/lib/python3.9/re.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?re.match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d952e61a-ed0c-483d-ac78-e43e3b6dbcf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Split: ['learn', 'php', 'from', 'guru99', 'and', 'make', 'study', 'easy']\n",
      "After Token: [('learn', 'JJ'), ('php', 'NN'), ('from', 'IN'), ('guru99', 'NN'), ('and', 'CC'), ('make', 'VB'), ('study', 'NN'), ('easy', 'JJ')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk learn/JJ)\n",
      "  (mychunk php/NN)\n",
      "  from/IN\n",
      "  (mychunk guru99/NN and/CC)\n",
      "  make/VB\n",
      "  (mychunk study/NN easy/JJ))\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk import RegexpParser\n",
    "text =\"learn php from guru99 and make study easy\".split()\n",
    "print(\"After Split:\",text)\n",
    "tokens_tag = pos_tag(text)\n",
    "print(\"After Token:\",tokens_tag)\n",
    "patterns= \"\"\"mychunk:{<NN.?>*<VBD.?>*<JJ.?>*<CC>?}\"\"\"\n",
    "chunker = RegexpParser(patterns)\n",
    "print(\"After Regex:\",chunker)\n",
    "output = chunker.parse(tokens_tag)\n",
    "print(\"After Chunking\",output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47516fc-e1b9-48b7-82a6-9650760b4560",
   "metadata": {},
   "outputs": [],
   "source": [
    "{((<NN|CD.?|RB>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "265446da-caaf-4bc8-82d2-5a47f8cd4413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my family used my spectranet and they don't want to help my ministry now it has finished. spectranet_ng abeg how will i change my password. \n",
      "\n",
      "spectranet_ng how can i get the freedom mifi in ajah today \n",
      "\n",
      "drolufunmilayo iconic_remi spectranet_ng \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for tweet in df.Text[:3]:\n",
    "    print(tweet,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464b78eb-1b9a-434e-be1b-ad25ead78742",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
