{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fba88215-aab4-4b5a-b8f5-80fc0d628f93",
   "metadata": {},
   "source": [
    "# Modeling: Aspect-Based Sentiment Analysis [Simplistic]\n",
    "\n",
    "**`Goal:`** \n",
    "\n",
    "Conduct ABSA using word relatedness and out-of-the-box [ABSA package by ScalaConsultants](https://github.com/ScalaConsultants/Aspect-Based-Sentiment-Analysis). This notebook is meant to serve as a start for tweet aspect annotation by getting as much of the aspects indicated and their corresponding sentiments. \n",
    "\n",
    "**Note:** Results will be crosschecked during the annotation phase!\n",
    "\n",
    "**`Process:`** \n",
    "1. List aspects (e.g. speed, price, reliability) determined from earlier data annotation phase\n",
    "2. Get nouns, adjectives and adverbs from the tweets as these will likely be the parts of speech making meaningful reference to aspects\n",
    "3. Check if each of the words from step 2 is very similar to any of the aspects (e.g. speed [aspect] and fast [word in tweet]) by computing relatedness score (via word embedding)\n",
    "4. If relatedness score is past a set thresholdhood, we assume the aspect was referenced in the tweet. Hence, note down that the aspect category was referenced in that given tweet and also note down the word (herein called aspect term) that implied the aspect\n",
    "6. Conduct ABSA using the ABSA package with the tweet and with the aspect term and note sentiment (positive, negative or neutral) towards the main aspect (price, speed, etc.)\n",
    "7. If multiple words make reference to a single aspect, find the average of their sentiments and use to assign a single sentiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d062c92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python -m spacy download en_core_web_md\n",
    "# python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c11b87e-67cd-44e7-a51c-e26a44c442f4",
   "metadata": {},
   "source": [
    "## 1. Library Importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ef903c6-788c-4790-ae9a-8e7eaf9dac83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-27 21:03:59.379723: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import aspect_based_sentiment_analysis as absa\n",
    "import nltk\n",
    "from nltk import pos_tag, RegexpParser\n",
    "\n",
    "#Packages for word relatedness computation\n",
    "import spacy\n",
    "spacy_nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "from itertools import product\n",
    "from cleantext import clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7f409f-5e2e-4898-b09e-e901ef6b7f29",
   "metadata": {},
   "source": [
    "## 2. Define function for annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da38bafd-1b5b-437c-bb0e-204c3a0b7dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at absa/classifier-rest-0.2 were not used when initializing BertABSClassifier: ['dropout_379']\n",
      "- This IS expected if you are initializing BertABSClassifier from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertABSClassifier from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of BertABSClassifier were not initialized from the model checkpoint at absa/classifier-rest-0.2 and are newly initialized: ['dropout_37']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#Load the model for ABSA modeling\n",
    "nlp = absa.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "633da9ab-e6c9-40ed-bd32-2070532dc6e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('price', price),\n",
       " ('speed', speed),\n",
       " ('reliability', reliability),\n",
       " ('coverage', coverage),\n",
       " ('customer service', customer service),\n",
       " ('trustworthiness', trustworthiness)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1. List aspects determined during the annotation phase\n",
    "    #Note: This might not be exhaustive! But it should cover most cases. It is also subjective!\n",
    "    #Also using synonyms of these words will likely yield different results\n",
    "aspects = ['price','speed','reliability','coverage', 'customer service', 'trustworthiness']\n",
    "\n",
    "#2. Pair aspects with their tokenized form to avoid recomputation in the ABSA phase below\n",
    "aspects_with_token = [] #List to store the pairing\n",
    "\n",
    "#Iterate through the aspects and compute their word vector using spacy\n",
    "for aspect in aspects:\n",
    "    aspects_with_token.append((aspect,spacy_nlp(aspect)))\n",
    "    \n",
    "aspects_with_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30eacece-4109-4f57-b5cc-d648d6c2ae7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_annotator(df, col_name, similarity_threshold = 0.6):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to perform unsupervised annotation of tweets based on process outlined above\n",
    "    \n",
    "    Inputs:\n",
    "        - df (pd DataFrame): A pandas dataframe to perform annotation on\n",
    "        - col_name (str): The specific column in the dataframe containing the tweets to use for annotation\n",
    "        - similarity_threshold (float): The threshold for aspect detection\n",
    "        \n",
    "    Output:\n",
    "        - absa_df (pd DataFrame): DataFrame containing the tweets and their ABSA annotation (if relevant)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #Set to store all seen words\n",
    "    seen_words = set()\n",
    "\n",
    "    #Set to store all aspect implying words found â€“ to avoid recomputing similarity scores\n",
    "    aspect_implying_words_glob = set()\n",
    "\n",
    "    #Dictionary categorizing all aspect-implying words into their relevant aspects\n",
    "    aspects_with_implying_words = {'price':set(),'speed':set(),'reliability':set(),'coverage':set(), \n",
    "                                   'customer service':set(),'trustworthiness':set()}\n",
    "\n",
    "    #List to store detected aspects and their sentiments\n",
    "    df_list = []\n",
    "\n",
    "    #Similarity threshold\n",
    "    sim_thresh = similarity_threshold\n",
    "\n",
    "    #Chunk tags to match â€“ i.e. parts of speech to extract\n",
    "    CHUNK_TAG = \"\"\"\n",
    "    MATCH: {<NN>+|<NN.*>+}\n",
    "    {<JJ.*>?}\n",
    "    {<RB.*>?}\n",
    "    \"\"\"\n",
    "\n",
    "    #Initialize chunk tag parser\n",
    "    cp = nltk.RegexpParser(CHUNK_TAG)\n",
    "\n",
    "    #Iterate through all the tweets\n",
    "    for tweet in df[col_name]:\n",
    "\n",
    "        #Set to store the detected aspects at the sentence level\n",
    "        # detected_aspects = set()\n",
    "\n",
    "        #Dictionary to store the sentiment value for each seen aspect\n",
    "        sentence_lvl_aspect_sentiment = {'price':[],'speed':[],'reliability':[],'coverage':[], \n",
    "                                         'customer service':[], 'trustworthiness':[]}\n",
    "\n",
    "        #Split the tweet into words\n",
    "        text = tweet.split()\n",
    "\n",
    "        #Tag the words with their part of speech\n",
    "        tokens_tag = pos_tag(text)\n",
    "\n",
    "        #Get the words with relevant POS (noun, adverbs, adjectives)\n",
    "        chunk_result = cp.parse(tokens_tag)\n",
    "\n",
    "        #Extract chunk results from tree into list \n",
    "        chunk_items = [list(n) for n in chunk_result if isinstance(n, nltk.tree.Tree)]\n",
    "\n",
    "        #Finally fuse/extract chunked words to get (noun) phrases, nouns, adverbs, adjectives\n",
    "        #1. List to store the words\n",
    "        matched_words = []\n",
    "\n",
    "        #2. Iterate through the chunked words lists and get the relevant words\n",
    "        for item in chunk_items:\n",
    "            if len(item) > 1:\n",
    "                full_string = []\n",
    "\n",
    "                for word in item:\n",
    "                    full_string.append(word[0])\n",
    "\n",
    "                matched_words.append(' '.join(full_string))\n",
    "\n",
    "            else:\n",
    "                matched_words.append(item[0][0])\n",
    "\n",
    "        #Iterate through all the words\n",
    "        for word_in_focus in matched_words:\n",
    "\n",
    "            #If the word has been seen before\n",
    "            if word_in_focus in seen_words:\n",
    "\n",
    "                #Check if the word is an aspect-implying word\n",
    "                if word_in_focus in aspect_implying_words_glob:\n",
    "\n",
    "                    #List to store all the aspects found to related to the certain word/token\n",
    "                    aspects_implied = []\n",
    "\n",
    "                #If it is an aspect-implying word, iterate through all the aspects\n",
    "                for aspect in aspects_with_implying_words.keys():\n",
    "                    \n",
    "                    #Check if the word_in_focus was noted as a word implying the aspect\n",
    "                    if word_in_focus in aspects_with_implying_words[aspect]:\n",
    "                        \n",
    "                        #Get all the aspects the word_in_focus implies\n",
    "                        aspects_implied.append(aspect)\n",
    "                        \n",
    "            \n",
    "            else:\n",
    "                continue\n",
    "                    \n",
    "         \n",
    "        #If the word hasn't been seen before\n",
    "        else:\n",
    "            \n",
    "            #Mark the word as seen now\n",
    "            seen_words.add(word_in_focus)\n",
    "                \n",
    "            #List to store all the aspects found to related to the certain word/token\n",
    "            #Ideally a given word won't imply multiple of the aspects as they are fairly independent\n",
    "            #-but just in case \n",
    "            aspects_implied = []\n",
    "\n",
    "            #Iterate through all the aspects\n",
    "            for aspect,asp_token in aspects_with_token:\n",
    "\n",
    "                #Translate word_in_focus to word vector\n",
    "                spacy_token = spacy_nlp(word_in_focus)\n",
    "\n",
    "                #Compute the similarity between the two word vectors (i.e. the two words)\n",
    "                #Round up to 1 d.p.\n",
    "                similarity_score = round(asp_token.similarity(spacy_token),1)\n",
    "\n",
    "                #If the max similarity score seen is greater than the threshold\n",
    "                if similarity_score >= sim_thresh:\n",
    "\n",
    "                    #Add the word to the set of all aspect-implying words seen\n",
    "                    aspect_implying_words_glob.add(word_in_focus)\n",
    "\n",
    "                    #Add the word to the dictionary of the relevant aspect word\n",
    "                    aspects_with_implying_words[aspect].add(word_in_focus)\n",
    "\n",
    "                    #Note that the aspect has been found in this particular sentence\n",
    "                    # detected_aspects.add(aspect)\n",
    "\n",
    "                    #Add the aspect to the list of aspects that the word_in_focus implies\n",
    "                    aspects_implied.append(aspect)\n",
    "\n",
    "\n",
    "                #If the word is not an aspect implying word, continue to next word\n",
    "                else:\n",
    "\n",
    "                    continue\n",
    "                \n",
    "        #Calculate the sentiment scores for the aspect_implying word in the current sentence\n",
    "        sentiment = nlp(tweet ,aspects = [word_in_focus])\n",
    "        sentiment_scores = sentiment.subtasks[word_in_focus].examples[0].scores\n",
    "\n",
    "        #Note down the scores for all the implied aspects\n",
    "        for aspect in aspects_implied:\n",
    "            sentence_lvl_aspect_sentiment[aspect].append(sentiment_scores)\n",
    "    \n",
    "        #List to store the detected aspects from the sentence\n",
    "        detected_aspects = []\n",
    "\n",
    "        #List to store the determined sentiments of the detected aspects\n",
    "        detected_sentiments = []\n",
    "\n",
    "        #Iterate through all the aspects\n",
    "        for aspect in sentence_lvl_aspect_sentiment.keys():\n",
    "\n",
    "            #If the aspect was detected in the sentence\n",
    "            if sentence_lvl_aspect_sentiment[aspect]:\n",
    "\n",
    "                #Record this\n",
    "                detected_aspects.append(aspect)\n",
    "\n",
    "                #Calculate the average sentiment scores across the different terms\n",
    "                avg_senti_score = np.array(sentence_lvl_aspect_sentiment[aspect]).mean(axis=0)\n",
    "\n",
    "                #Get the sentiment category (neutral,negative,positive) with the largest probability\n",
    "                max_idx = np.argmax(avg_senti_score)\n",
    "\n",
    "                if max_idx == 2:\n",
    "\n",
    "                    detected_sentiments.append(\"Positive\")\n",
    "\n",
    "                elif max_idx == 1:\n",
    "\n",
    "                    detected_sentiments.append(\"Negative\")\n",
    "\n",
    "                else:\n",
    "\n",
    "                    detected_sentiments.append(\"Neutral\")\n",
    "\n",
    "        #Add the detected aspects and sentiments from the sentence to the list\n",
    "        if detected_aspects:\n",
    "            df_list.append([tweet,detected_aspects,detected_sentiments])\n",
    "        else:\n",
    "            df_list.append([tweet,None,None])\n",
    "\n",
    "\n",
    "    absa_df = pd.DataFrame(df_list, \n",
    "                       columns=[col_name,'Detected aspects','Corresponding sentiment'])\n",
    "    \n",
    "    return absa_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585f571d-86ad-4897-8b0c-5c63458921e5",
   "metadata": {},
   "source": [
    "## 3. Annotating the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f196af6-46fc-46fc-b124-d0c0f8a490e6",
   "metadata": {},
   "source": [
    "### a. More Nigerian ISP data for annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31a996b-db0f-4053-a8af-050a83f2a782",
   "metadata": {},
   "source": [
    "#### (i) Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87b6aef2-04e0-41f2-ba13-b0f06be1c197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Cleaned text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spectranet_ng is this even fair? i won't renew...</td>\n",
       "      <td>spectranetng is this even fair i wont renew ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eniolashitta youtube is where spectranet start...</td>\n",
       "      <td>eniolashitta youtube is where spectranet start...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>oluwadamilolaog spectranet_ng my second device...</td>\n",
       "      <td>oluwadamilolaog spectranetng my second device ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mtnng globacomnigeria gloworld airtelnigeria e...</td>\n",
       "      <td>mtnng globacomnigeria gloworld airtelnigeria e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>igalaman tizeti no one. and they will still co...</td>\n",
       "      <td>igalaman tizeti no one and they will still col...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  spectranet_ng is this even fair? i won't renew...   \n",
       "1  eniolashitta youtube is where spectranet start...   \n",
       "2  oluwadamilolaog spectranet_ng my second device...   \n",
       "3  mtnng globacomnigeria gloworld airtelnigeria e...   \n",
       "4  igalaman tizeti no one. and they will still co...   \n",
       "\n",
       "                                        Cleaned text  \n",
       "0  spectranetng is this even fair i wont renew ne...  \n",
       "1  eniolashitta youtube is where spectranet start...  \n",
       "2  oluwadamilolaog spectranetng my second device ...  \n",
       "3  mtnng globacomnigeria gloworld airtelnigeria e...  \n",
       "4  igalaman tizeti no one and they will still col...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_annotations = pd.read_csv('../data/interim/new_text_for_absa_annotation2.csv')\n",
    "new_annotations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed94bd8c-c91b-457f-943c-8fd96972c91e",
   "metadata": {},
   "source": [
    "#### (ii) Perform the ABSA annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d966d9ad-e82d-4ed1-ac16-325af4a6e8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/koredeakande/opt/anaconda3/envs/capstone/lib/python3.7/site-packages/ipykernel_launcher.py:126: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n"
     ]
    }
   ],
   "source": [
    "newly_annotated_df = tweet_annotator(new_annotations, 'Cleaned text')\n",
    "newly_annotated_df.insert(0,'Tweets',new_annotations.Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a2cea0ec-bd89-403f-85c2-464964aec4a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Cleaned tweets</th>\n",
       "      <th>Detected aspects</th>\n",
       "      <th>Corresponding sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spectranet_ng is this even fair? i won't renew next month and you people should not even bother calling me. i will curse you!</td>\n",
       "      <td>spectranetng is this even fair i wont renew next month and you people should not even bother calling me i will curse you</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eniolashitta youtube is where spectranet starts to smile cos data will disappear fast fast ðŸ˜­ðŸ˜­</td>\n",
       "      <td>eniolashitta youtube is where spectranet starts to smile cos data will disappear fast fast</td>\n",
       "      <td>[speed]</td>\n",
       "      <td>[Negative]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>oluwadamilolaog spectranet_ng my second device , the big one .</td>\n",
       "      <td>oluwadamilolaog spectranetng my second device the big one</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mtnng globacomnigeria gloworld airtelnigeria etisalat_care spectranet_ng can we please get a 50% cut off price on data? it's a crucial time now. we need a pay cut. #weneeddatapaycut #paycutdata segalink gidi_traffic tosinolugbenga omojuwa drjoeabah seunkuti housengr</td>\n",
       "      <td>mtnng globacomnigeria gloworld airtelnigeria etisalatcare spectranetng can we please get a 50 cut off price on data its a crucial time now we need a pay cut weneeddatapaycut paycutdata segalink giditraffic tosinolugbenga omojuwa drjoeabah seunkuti housengr</td>\n",
       "      <td>[price]</td>\n",
       "      <td>[Negative]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>igalaman tizeti no one. and they will still collect full money</td>\n",
       "      <td>igalaman tizeti no one and they will still collect full money</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>633</th>\n",
       "      <td>spectranet_ng m_customerfirst sure. thanks!</td>\n",
       "      <td>spectranetng mcustomerfirst sure thanks</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634</th>\n",
       "      <td>riqueza_cakes get spectranet then .</td>\n",
       "      <td>riquezacakes get spectranet then</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>635</th>\n",
       "      <td>spectranet is always terrible at night. fix up ffs spectranet_ng</td>\n",
       "      <td>spectranet is always terrible at night fix up ffs spectranetng</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>636</th>\n",
       "      <td>spectranet_ng are we getting 100% today ?</td>\n",
       "      <td>spectranetng are we getting 100 today</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>i don't know why spectranet_ng has the shittiest customer service like it's just wild</td>\n",
       "      <td>i dont know why spectranetng has the shittiest customer service like its just wild</td>\n",
       "      <td>[customer service]</td>\n",
       "      <td>[Negative]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>638 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                         Tweets  \\\n",
       "0                                                                                                                                                 spectranet_ng is this even fair? i won't renew next month and you people should not even bother calling me. i will curse you!   \n",
       "1                                                                                                                                                                                 eniolashitta youtube is where spectranet starts to smile cos data will disappear fast fast ðŸ˜­ðŸ˜­   \n",
       "2                                                                                                                                                                                                                oluwadamilolaog spectranet_ng my second device , the big one .   \n",
       "3    mtnng globacomnigeria gloworld airtelnigeria etisalat_care spectranet_ng can we please get a 50% cut off price on data? it's a crucial time now. we need a pay cut. #weneeddatapaycut #paycutdata segalink gidi_traffic tosinolugbenga omojuwa drjoeabah seunkuti housengr   \n",
       "4                                                                                                                                                                                                                igalaman tizeti no one. and they will still collect full money   \n",
       "..                                                                                                                                                                                                                                                                          ...   \n",
       "633                                                                                                                                                                                                                                 spectranet_ng m_customerfirst sure. thanks!   \n",
       "634                                                                                                                                                                                                                                         riqueza_cakes get spectranet then .   \n",
       "635                                                                                                                                                                                                            spectranet is always terrible at night. fix up ffs spectranet_ng   \n",
       "636                                                                                                                                                                                                                                   spectranet_ng are we getting 100% today ?   \n",
       "637                                                                                                                                                                                       i don't know why spectranet_ng has the shittiest customer service like it's just wild   \n",
       "\n",
       "                                                                                                                                                                                                                                                       Cleaned tweets  \\\n",
       "0                                                                                                                                            spectranetng is this even fair i wont renew next month and you people should not even bother calling me i will curse you   \n",
       "1                                                                                                                                                                         eniolashitta youtube is where spectranet starts to smile cos data will disappear fast fast    \n",
       "2                                                                                                                                                                                                           oluwadamilolaog spectranetng my second device the big one   \n",
       "3    mtnng globacomnigeria gloworld airtelnigeria etisalatcare spectranetng can we please get a 50 cut off price on data its a crucial time now we need a pay cut weneeddatapaycut paycutdata segalink giditraffic tosinolugbenga omojuwa drjoeabah seunkuti housengr   \n",
       "4                                                                                                                                                                                                       igalaman tizeti no one and they will still collect full money   \n",
       "..                                                                                                                                                                                                                                                                ...   \n",
       "633                                                                                                                                                                                                                           spectranetng mcustomerfirst sure thanks   \n",
       "634                                                                                                                                                                                                                                  riquezacakes get spectranet then   \n",
       "635                                                                                                                                                                                                    spectranet is always terrible at night fix up ffs spectranetng   \n",
       "636                                                                                                                                                                                                                             spectranetng are we getting 100 today   \n",
       "637                                                                                                                                                                                i dont know why spectranetng has the shittiest customer service like its just wild   \n",
       "\n",
       "       Detected aspects Corresponding sentiment  \n",
       "0                  None                    None  \n",
       "1               [speed]              [Negative]  \n",
       "2                  None                    None  \n",
       "3               [price]              [Negative]  \n",
       "4                  None                    None  \n",
       "..                  ...                     ...  \n",
       "633                None                    None  \n",
       "634                None                    None  \n",
       "635                None                    None  \n",
       "636                None                    None  \n",
       "637  [customer service]              [Negative]  \n",
       "\n",
       "[638 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(newly_annotated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "838564eb-e1d4-42ef-a3a3-c2fe67cb138b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write annotated dataframe to CSV\n",
    "# newly_annotated_df.to_csv('../data/model-generated/tweet_absa_second_annotation.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8ed09f-b695-48a4-9057-4b15eb24d7c5",
   "metadata": {},
   "source": [
    "### b. Non-Nigerian ISP data for annotation (Analogous data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e883abd4-a6b5-4a8c-b0e5-37354e227e17",
   "metadata": {},
   "source": [
    "#### (i) Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "684632f8-2e73-4bfd-8359-97cdcb24d3fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Cleaned text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>iamrenike: the sexual tension between spectran...</td>\n",
       "      <td>iamrenike the sexual tension between spectrane...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spectranet or smile? which is more reliable?</td>\n",
       "      <td>spectranet or smile which is more reliable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spectranet, and glo dey cook me seriously for ...</td>\n",
       "      <td>spectranet and glo dey cook me seriously for here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spectranet offer state-of-the-art dedicated li...</td>\n",
       "      <td>spectranet offer stateoftheart dedicated link ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rhanty - lmao make i run the playstation plus....</td>\n",
       "      <td>rhanty lmao make i run the playstation plus sp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  iamrenike: the sexual tension between spectran...   \n",
       "1       spectranet or smile? which is more reliable?   \n",
       "2  spectranet, and glo dey cook me seriously for ...   \n",
       "3  spectranet offer state-of-the-art dedicated li...   \n",
       "4  rhanty - lmao make i run the playstation plus....   \n",
       "\n",
       "                                        Cleaned text  \n",
       "0  iamrenike the sexual tension between spectrane...  \n",
       "1         spectranet or smile which is more reliable  \n",
       "2  spectranet and glo dey cook me seriously for here  \n",
       "3  spectranet offer stateoftheart dedicated link ...  \n",
       "4  rhanty lmao make i run the playstation plus sp...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogous_tweets = pd.read_csv('../data/interim/cleaned_analogous_tweets.csv')\n",
    "analogous_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1028a2e1-41ad-443d-be58-e54c9e435554",
   "metadata": {},
   "source": [
    "#### (ii) Drop NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3c6d6310-d77a-42c4-b87b-115c8fd90083",
   "metadata": {},
   "outputs": [],
   "source": [
    "analogous_tweets = analogous_tweets.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b199b74a-c86a-47b4-885b-b7978765b9c6",
   "metadata": {},
   "source": [
    "#### (iii) Perform the ABSA annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "08895570-2220-446f-9de9-1221285b9475",
   "metadata": {},
   "outputs": [],
   "source": [
    "analogous_annotated = tweet_annotator(analogous_tweets, 'Cleaned text')\n",
    "analogous_annotated.insert(0,'Tweets',analogous_tweets.Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "778451fd-8019-4ccd-a86d-0968f9e1e675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Cleaned text</th>\n",
       "      <th>Detected aspects</th>\n",
       "      <th>Corresponding sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>iamrenike: the sexual tension between spectran...</td>\n",
       "      <td>iamrenike the sexual tension between spectrane...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spectranet or smile? which is more reliable?</td>\n",
       "      <td>spectranet or smile which is more reliable</td>\n",
       "      <td>[reliability, customer service]</td>\n",
       "      <td>[Positive, Positive]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spectranet, and glo dey cook me seriously for ...</td>\n",
       "      <td>spectranet and glo dey cook me seriously for here</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spectranet offer state-of-the-art dedicated li...</td>\n",
       "      <td>spectranet offer stateoftheart dedicated link ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rhanty - lmao make i run the playstation plus....</td>\n",
       "      <td>rhanty lmao make i run the playstation plus sp...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tweets  \\\n",
       "0  iamrenike: the sexual tension between spectran...   \n",
       "1       spectranet or smile? which is more reliable?   \n",
       "2  spectranet, and glo dey cook me seriously for ...   \n",
       "3  spectranet offer state-of-the-art dedicated li...   \n",
       "4  rhanty - lmao make i run the playstation plus....   \n",
       "\n",
       "                                        Cleaned text  \\\n",
       "0  iamrenike the sexual tension between spectrane...   \n",
       "1         spectranet or smile which is more reliable   \n",
       "2  spectranet and glo dey cook me seriously for here   \n",
       "3  spectranet offer stateoftheart dedicated link ...   \n",
       "4  rhanty lmao make i run the playstation plus sp...   \n",
       "\n",
       "                  Detected aspects Corresponding sentiment  \n",
       "0                             None                    None  \n",
       "1  [reliability, customer service]    [Positive, Positive]  \n",
       "2                             None                    None  \n",
       "3                             None                    None  \n",
       "4                             None                    None  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogous_annotated.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32338645-1d60-48d4-a8e8-b55a8f4dc25e",
   "metadata": {},
   "source": [
    "#Write annotated dataframe to CSV\n",
    "analogous_annotated.to_csv('../data/model-generated/annotated_analogous_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b206a10b-4172-4de3-9f2a-5fab95c48c3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
