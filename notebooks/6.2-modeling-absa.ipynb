{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb98231-329c-4ee4-965d-e732964748ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037a9e58-f596-42d3-9665-633c71e30836",
   "metadata": {},
   "outputs": [],
   "source": [
    "python train.py --model_name bert_spc --dataset twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfc3b5a2-8655-4db5-9fcb-8e753d92d7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37e87fe1-2db0-4d52-a1ca-2f3318876ab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mweight\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msize_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mignore_index\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreduction\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'mean'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "This criterion computes the cross entropy loss between input and target.\n",
       "\n",
       "It is useful when training a classification problem with `C` classes.\n",
       "If provided, the optional argument :attr:`weight` should be a 1D `Tensor`\n",
       "assigning weight to each of the classes.\n",
       "This is particularly useful when you have an unbalanced training set.\n",
       "\n",
       "The `input` is expected to contain raw, unnormalized scores for each class.\n",
       "`input` has to be a Tensor of size either :math:`(minibatch, C)` or\n",
       ":math:`(minibatch, C, d_1, d_2, ..., d_K)` with :math:`K \\geq 1` for the\n",
       "`K`-dimensional case. The latter is useful for higher dimension inputs, such\n",
       "as computing cross entropy loss per-pixel for 2D images.\n",
       "\n",
       "The `target` that this criterion expects should contain either:\n",
       "\n",
       "- Class indices in the range :math:`[0, C-1]` where :math:`C` is the number of classes; if\n",
       "  `ignore_index` is specified, this loss also accepts this class index (this index\n",
       "  may not necessarily be in the class range). The unreduced (i.e. with :attr:`reduction`\n",
       "  set to ``'none'``) loss for this case can be described as:\n",
       "\n",
       "  .. math::\n",
       "      \\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\n",
       "      l_n = - w_{y_n} \\log \\frac{\\exp(x_{n,y_n})}{\\sum_{c=1}^C \\exp(x_{n,c})}\n",
       "      \\cdot \\mathbb{1}\\{y_n \\not= \\text{ignore\\_index}\\}\n",
       "\n",
       "  where :math:`x` is the input, :math:`y` is the target, :math:`w` is the weight,\n",
       "  :math:`C` is the number of classes, and :math:`N` spans the minibatch dimension as well as\n",
       "  :math:`d_1, ..., d_k` for the `K`-dimensional case. If\n",
       "  :attr:`reduction` is not ``'none'`` (default ``'mean'``), then\n",
       "\n",
       "  .. math::\n",
       "      \\ell(x, y) = \\begin{cases}\n",
       "          \\sum_{n=1}^N \\frac{1}{\\sum_{n=1}^N w_{y_n} \\cdot \\mathbb{1}\\{y_n \\not= \\text{ignore\\_index}\\}} l_n, &\n",
       "           \\text{if reduction} = \\text{`mean';}\\\\\n",
       "            \\sum_{n=1}^N l_n,  &\n",
       "            \\text{if reduction} = \\text{`sum'.}\n",
       "        \\end{cases}\n",
       "\n",
       "  Note that this case is equivalent to the combination of :class:`~torch.nn.LogSoftmax` and\n",
       "  :class:`~torch.nn.NLLLoss`.\n",
       "\n",
       "- Probabilities for each class; useful when labels beyond a single class per minibatch item\n",
       "  are required, such as for blended labels, label smoothing, etc. The unreduced (i.e. with\n",
       "  :attr:`reduction` set to ``'none'``) loss for this case can be described as:\n",
       "\n",
       "  .. math::\n",
       "      \\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\n",
       "      l_n = - \\sum_{c=1}^C w_c \\log \\frac{\\exp(x_{n,c})}{\\exp(\\sum_{i=1}^C x_{n,i})} y_{n,c}\n",
       "\n",
       "  where :math:`x` is the input, :math:`y` is the target, :math:`w` is the weight,\n",
       "  :math:`C` is the number of classes, and :math:`N` spans the minibatch dimension as well as\n",
       "  :math:`d_1, ..., d_k` for the `K`-dimensional case. If\n",
       "  :attr:`reduction` is not ``'none'`` (default ``'mean'``), then\n",
       "\n",
       "  .. math::\n",
       "      \\ell(x, y) = \\begin{cases}\n",
       "          \\frac{\\sum_{n=1}^N l_n}{N}, &\n",
       "           \\text{if reduction} = \\text{`mean';}\\\\\n",
       "            \\sum_{n=1}^N l_n,  &\n",
       "            \\text{if reduction} = \\text{`sum'.}\n",
       "        \\end{cases}\n",
       "\n",
       ".. note::\n",
       "    The performance of this criterion is generally better when `target` contains class\n",
       "    indices, as this allows for optimized computation. Consider providing `target` as\n",
       "    class probabilities only when a single class label per minibatch item is too restrictive.\n",
       "\n",
       "Args:\n",
       "    weight (Tensor, optional): a manual rescaling weight given to each class.\n",
       "        If given, has to be a Tensor of size `C`\n",
       "    size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n",
       "        the losses are averaged over each loss element in the batch. Note that for\n",
       "        some losses, there are multiple elements per sample. If the field :attr:`size_average`\n",
       "        is set to ``False``, the losses are instead summed for each minibatch. Ignored\n",
       "        when :attr:`reduce` is ``False``. Default: ``True``\n",
       "    ignore_index (int, optional): Specifies a target value that is ignored\n",
       "        and does not contribute to the input gradient. When :attr:`size_average` is\n",
       "        ``True``, the loss is averaged over non-ignored targets. Note that\n",
       "        :attr:`ignore_index` is only applicable when the target contains class indices.\n",
       "    reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n",
       "        losses are averaged or summed over observations for each minibatch depending\n",
       "        on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n",
       "        batch element instead and ignores :attr:`size_average`. Default: ``True``\n",
       "    reduction (string, optional): Specifies the reduction to apply to the output:\n",
       "        ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will\n",
       "        be applied, ``'mean'``: the weighted mean of the output is taken,\n",
       "        ``'sum'``: the output will be summed. Note: :attr:`size_average`\n",
       "        and :attr:`reduce` are in the process of being deprecated, and in\n",
       "        the meantime, specifying either of those two args will override\n",
       "        :attr:`reduction`. Default: ``'mean'``\n",
       "    label_smoothing (float, optional): A float in [0.0, 1.0]. Specifies the amount\n",
       "        of smoothing when computing the loss, where 0.0 means no smoothing. The targets\n",
       "        become a mixture of the original ground truth and a uniform distribution as described in\n",
       "        `Rethinking the Inception Architecture for Computer Vision <https://arxiv.org/abs/1512.00567>`__. Default: :math:`0.0`.\n",
       "\n",
       "Shape:\n",
       "    - Input: :math:`(N, C)` where `C = number of classes`, or\n",
       "      :math:`(N, C, d_1, d_2, ..., d_K)` with :math:`K \\geq 1`\n",
       "      in the case of `K`-dimensional loss.\n",
       "    - Target: If containing class indices, shape :math:`(N)` where each value is\n",
       "      :math:`0 \\leq \\text{targets}[i] \\leq C-1`, or :math:`(N, d_1, d_2, ..., d_K)` with\n",
       "      :math:`K \\geq 1` in the case of K-dimensional loss. If containing class probabilities,\n",
       "      same shape as the input.\n",
       "    - Output: If :attr:`reduction` is ``'none'``, shape :math:`(N)` or\n",
       "      :math:`(N, d_1, d_2, ..., d_K)` with :math:`K \\geq 1` in the case of K-dimensional loss.\n",
       "      Otherwise, scalar.\n",
       "\n",
       "Examples::\n",
       "\n",
       "    >>> # Example of target with class indices\n",
       "    >>> loss = nn.CrossEntropyLoss()\n",
       "    >>> input = torch.randn(3, 5, requires_grad=True)\n",
       "    >>> target = torch.empty(3, dtype=torch.long).random_(5)\n",
       "    >>> output = loss(input, target)\n",
       "    >>> output.backward()\n",
       "    >>>\n",
       "    >>> # Example of target with class probabilities\n",
       "    >>> input = torch.randn(3, 5, requires_grad=True)\n",
       "    >>> target = torch.randn(3, 5).softmax(dim=1)\n",
       "    >>> output = loss(input, target)\n",
       "    >>> output.backward()\n",
       "\u001b[0;31mInit docstring:\u001b[0m Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/opt/anaconda3/envs/jupyterlab/lib/python3.9/site-packages/torch/nn/modules/loss.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?nn.CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c92a4d4-34e7-4993-98a1-7166495aa513",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
