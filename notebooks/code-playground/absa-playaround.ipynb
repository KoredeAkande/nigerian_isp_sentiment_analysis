{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "247a68b2-342d-4000-aac5-af003c90a518",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag, RegexpParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "e4bd6f6e-9a5c-4987-8983-602e7f9b0d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jon3ro', 'spectranet', 'is', 'actually', 'more', 'expensive', 'and', 'have', 'a', 'shitty', 'service']\n",
      "[('jon3ro', 'NN'), ('spectranet', 'NN'), ('is', 'VBZ'), ('actually', 'RB'), ('more', 'RBR'), ('expensive', 'JJ'), ('and', 'CC'), ('have', 'VBP'), ('a', 'DT'), ('shitty', 'JJ'), ('service', 'NN')]\n",
      "(S\n",
      "  (MATCH jon3ro/NN spectranet/NN)\n",
      "  is/VBZ\n",
      "  (MATCH actually/RB)\n",
      "  (MATCH more/RBR)\n",
      "  (MATCH expensive/JJ)\n",
      "  and/CC\n",
      "  have/VBP\n",
      "  a/DT\n",
      "  (MATCH shitty/JJ)\n",
      "  (MATCH service/NN))\n"
     ]
    }
   ],
   "source": [
    "text = \"jon3ro spectranet is actually more expensive and have a shitty service\"\n",
    "tokens = text.split()\n",
    "print(tokens)\n",
    "tag = nltk.pos_tag(tokens)\n",
    "print(tag)\n",
    "grammar = \"\"\"\n",
    "MATCH: {<NN>+|<NN.*>+}\n",
    "{<JJ.*>?}\n",
    "{<RB.*>?}\n",
    "\"\"\"\n",
    "cp  =nltk.RegexpParser(grammar)\n",
    "result = cp.parse(tag)\n",
    "print(result)\n",
    "#result.draw()    # It wil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ab3293c8-6f4a-4d3c-9cd7-06144944783c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('customer', 'NN'), ('service', 'NN')],\n",
       " [('whack.', 'JJ')],\n",
       " [('Food', 'NNP'), ('delivery', 'NN')],\n",
       " [('really', 'RB')],\n",
       " [('poor', 'JJ')],\n",
       " [('network', 'NN'), ('coverage', 'NN')]]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items = \n",
    "items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "6d37bea9-bab1-402e-9eb3-fb0be8743734",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_words = []\n",
    "\n",
    "for item in items:\n",
    "    if len(item) > 1:\n",
    "        full_string = []\n",
    "        \n",
    "        for word in item:\n",
    "            full_string.append(word[0])\n",
    "            \n",
    "        matched_words.append(' '.join(full_string))\n",
    "        \n",
    "    else:\n",
    "        matched_words.append(item[0][0])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "90382dc2-aae8-40a9-8fd5-50b04f17ed2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['customer service',\n",
       " 'whack.',\n",
       " 'Food delivery',\n",
       " 'really',\n",
       " 'poor',\n",
       " 'network coverage']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matched_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "cd7c7528-7a33-471b-a373-4316ed165e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "item = []\n",
    "\n",
    "for n in result:\n",
    "    if isinstance(n, nltk.tree.Tree):               \n",
    "        if n.label() == 'MATCH':\n",
    "            item.append(list(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9027115a-862f-48b2-888d-2b6b61323450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Tree.leaf_treeposition of Tree('S', [Tree('NP', [('customer', 'NN'), ('service', 'NN')]), ('is', 'VBZ'), ('whack', 'JJ')])>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.leaf_treeposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd24c6a6-5d4f-4ad7-b2c1-c6e0d2626279",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acf6c1f2-d21e-49bb-aee0-73ba346d29a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aspect_based_sentiment_analysis as absa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0657af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set to store all seen words\n",
    "seen_words = set()\n",
    "\n",
    "#Set to store all aspect implying words found to avoid recomputing similarity words\n",
    "aspect_implying_words_glob = set()\n",
    "\n",
    "#Dictionary containing sets to track all seen aspect-implying words\n",
    "aspects_with_implying_words = {'price':set(),'speed':set(),'reliability':set(),\n",
    "                               'coverage':set(), 'customer service':set()}\n",
    "\n",
    "#Similarity threshold\n",
    "sim_thresh = 0.6\n",
    "\n",
    "#Iterate through all the tweets\n",
    "for tweet in df.Text:\n",
    "    \n",
    "    #Dictionary to store the sentiment value for each seen aspect\n",
    "    sentence_lvl_aspect_sentiment = {'price':[],'speed':[],'reliability':[],\n",
    "                                     'coverage':[], 'customer service':[]}\n",
    "        \n",
    "    #Split the tweet into words\n",
    "    text = tweet.split()\n",
    "\n",
    "    #Tag words with part of speech\n",
    "    tokens_tag = pos_tag(text)\n",
    "\n",
    "    #Iterate through all the tagged words\n",
    "    for token in tokens_tag:\n",
    "        \n",
    "        #If the word has been seen before\n",
    "        if token[0] in seen_words:\n",
    "            \n",
    "            \n",
    "            #List to store all the aspects found to related to the certain word/token\n",
    "            aspects_implied = []\n",
    "            \n",
    "            \n",
    "            sksksksls\n",
    "            \n",
    "        else:\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #Check if the tagged word is a noun, adjective or adverb\n",
    "        regex_match = re.match('NN.?|JJ.?|RB.?',token[1])\n",
    "\n",
    "        #If it is one of the mentioned parts of speech\n",
    "        if regex_match:\n",
    "            \n",
    "            #Get the word\n",
    "            word_in_focus = token[0]\n",
    "        \n",
    "            #If the word has not been before\n",
    "            if word_in_focus not in seen_words:\n",
    "            \n",
    "                #Iterate through all the aspects and compute similarity/relatedness\n",
    "                for aspect in aspects:\n",
    "\n",
    "                    #Look up the words on wordnet â€“ this gets multiple versions of the word\n",
    "                    sem1, sem2 = wn.synsets(aspect), wn.synsets(word_in_focus)\n",
    "\n",
    "                    #Iterate through different permutations of the versions of the words\n",
    "                    #and get the max similarity score seen\n",
    "                    maxscore = 0\n",
    "                    \n",
    "                    #Get the max similarity score between the words\n",
    "                    for i,j in list(product(*[sem1,sem2])):\n",
    "                      score = i.wup_similarity(j) # Wu-Palmer Similarity\n",
    "                      maxscore = score if maxscore < score else maxscore\n",
    "\n",
    "                    #If the max similarity score seen is greater than the threshold\n",
    "                    if maxscore > sim_thresh:\n",
    "\n",
    "                        #Add the word to the set of all aspect-implying words seen\n",
    "                        aspect_implying_words_glob.add(word_in_focus)\n",
    "\n",
    "                        #Add the word to the dictionary of the relevant aspect word\n",
    "                        aspects_with_implying_words[aspect].add(word_in_focus)\n",
    "                        \n",
    "                        #Add the aspect to the list of aspects that the word_in_focus implies\n",
    "                        aspects_implied.append(aspect)\n",
    "                \n",
    "                #Add the word to the seen words\n",
    "                seen_words.add(word_in_focus)\n",
    "                \n",
    "            else:\n",
    "        \n",
    "        else:\n",
    "            \n",
    "                \n",
    "                \n",
    "            \n",
    "            #Calculate the sentiment for the aspect_implying word in the current sentence\n",
    "            sentiment = nlp(tweet ,aspects = [aspect_term])\n",
    "\n",
    "            #Append the sentiment score to the sentiment scores list\n",
    "            term_sentiments.append(np.array(sent.subtasks[aspect_term].examples[0].scores))\n",
    "                \n",
    "                    \n",
    "                \n",
    "                \n",
    "                \n",
    "                # 1. Iterate through each word\n",
    "                # 2. Check word with aspects\n",
    "                # 3. If word similar to aspect, compute \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "        \n",
    "text =\"learn php from guru99 and make study easy\".split()\n",
    "print(\"After Split:\",text)\n",
    "tokens_tag = pos_tag(text)\n",
    "print(\"After Token:\",tokens_tag)\n",
    "patterns= \"\"\"mychunk:{<NN.?>*<VBD.?>*<JJ.?>*<CC>?}\"\"\"\n",
    "chunker = RegexpParser(patterns)\n",
    "print(\"After Regex:\",chunker)\n",
    "output = chunker.parse(tokens_tag)\n",
    "print(\"After Chunking\",output)\n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced661c7",
   "metadata": {},
   "source": [
    "### Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6f304813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "  \n",
    "spacy_nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "036ec938-60d9-4edb-af61-8e8ae1cddfa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: 0.5153175265355375\n"
     ]
    }
   ],
   "source": [
    "token1 = spacy_nlp('network reliability')\n",
    "token2 = spacy_nlp('customer service')\n",
    "print(\"Similarity:\", token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7d1aa1-169e-4ad0-b187-45eaed0081d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "034901bc-bc4b-4bf4-991b-5afffcef25a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f94951df-160f-44fe-a529-5b0e9c1ef1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c71388a-cd98-436b-96f7-e3f3b93eeac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mBertForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Instantiate a pretrained pytorch model from a pre-trained model configuration.\n",
       "\n",
       "The model is set in evaluation mode by default using ``model.eval()`` (Dropout modules are deactivated). To\n",
       "train the model, you should first set it back in training mode with ``model.train()``.\n",
       "\n",
       "The warning `Weights from XXX not initialized from pretrained model` means that the weights of XXX do not come\n",
       "pretrained with the rest of the model. It is up to you to train those weights with a downstream fine-tuning\n",
       "task.\n",
       "\n",
       "The warning `Weights from XXX not used in YYY` means that the layer XXX is not used by YYY, therefore those\n",
       "weights are discarded.\n",
       "\n",
       "Parameters:\n",
       "    pretrained_model_name_or_path (:obj:`str` or :obj:`os.PathLike`, `optional`):\n",
       "        Can be either:\n",
       "\n",
       "            - A string, the `model id` of a pretrained model hosted inside a model repo on huggingface.co.\n",
       "              Valid model ids can be located at the root-level, like ``bert-base-uncased``, or namespaced under\n",
       "              a user or organization name, like ``dbmdz/bert-base-german-cased``.\n",
       "            - A path to a `directory` containing model weights saved using\n",
       "              :func:`~transformers.PreTrainedModel.save_pretrained`, e.g., ``./my_model_directory/``.\n",
       "            - A path or url to a `tensorflow index checkpoint file` (e.g, ``./tf_model/model.ckpt.index``). In\n",
       "              this case, ``from_tf`` should be set to :obj:`True` and a configuration object should be provided\n",
       "              as ``config`` argument. This loading path is slower than converting the TensorFlow checkpoint in\n",
       "              a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.\n",
       "            - A path or url to a model folder containing a `flax checkpoint file` in `.msgpack` format (e.g,\n",
       "              ``./flax_model/`` containing ``flax_model.msgpack``). In this case, ``from_flax`` should be set\n",
       "              to :obj:`True`.\n",
       "            - :obj:`None` if you are both providing the configuration and state dictionary (resp. with keyword\n",
       "              arguments ``config`` and ``state_dict``).\n",
       "    model_args (sequence of positional arguments, `optional`):\n",
       "        All remaning positional arguments will be passed to the underlying model's ``__init__`` method.\n",
       "    config (:obj:`Union[PretrainedConfig, str, os.PathLike]`, `optional`):\n",
       "        Can be either:\n",
       "\n",
       "            - an instance of a class derived from :class:`~transformers.PretrainedConfig`,\n",
       "            - a string or path valid as input to :func:`~transformers.PretrainedConfig.from_pretrained`.\n",
       "\n",
       "        Configuration for the model to use instead of an automatically loaded configuation. Configuration can\n",
       "        be automatically loaded when:\n",
       "\n",
       "            - The model is a model provided by the library (loaded with the `model id` string of a pretrained\n",
       "              model).\n",
       "            - The model was saved using :func:`~transformers.PreTrainedModel.save_pretrained` and is reloaded\n",
       "              by supplying the save directory.\n",
       "            - The model is loaded by supplying a local directory as ``pretrained_model_name_or_path`` and a\n",
       "              configuration JSON file named `config.json` is found in the directory.\n",
       "    state_dict (:obj:`Dict[str, torch.Tensor]`, `optional`):\n",
       "        A state dictionary to use instead of a state dictionary loaded from saved weights file.\n",
       "\n",
       "        This option can be used if you want to create a model from a pretrained configuration but load your own\n",
       "        weights. In this case though, you should check if using\n",
       "        :func:`~transformers.PreTrainedModel.save_pretrained` and\n",
       "        :func:`~transformers.PreTrainedModel.from_pretrained` is not a simpler option.\n",
       "    cache_dir (:obj:`Union[str, os.PathLike]`, `optional`):\n",
       "        Path to a directory in which a downloaded pretrained model configuration should be cached if the\n",
       "        standard cache should not be used.\n",
       "    from_tf (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
       "        Load the model weights from a TensorFlow checkpoint save file (see docstring of\n",
       "        ``pretrained_model_name_or_path`` argument).\n",
       "    from_flax (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
       "        Load the model weights from a Flax checkpoint save file (see docstring of\n",
       "        ``pretrained_model_name_or_path`` argument).\n",
       "    force_download (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
       "        Whether or not to force the (re-)download of the model weights and configuration files, overriding the\n",
       "        cached versions if they exist.\n",
       "    resume_download (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
       "        Whether or not to delete incompletely received files. Will attempt to resume the download if such a\n",
       "        file exists.\n",
       "    proxies (:obj:`Dict[str, str], `optional`):\n",
       "        A dictionary of proxy servers to use by protocol or endpoint, e.g., :obj:`{'http': 'foo.bar:3128',\n",
       "        'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n",
       "    output_loading_info(:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
       "        Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.\n",
       "    local_files_only(:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
       "        Whether or not to only look at local files (i.e., do not try to download the model).\n",
       "    use_auth_token (:obj:`str` or `bool`, `optional`):\n",
       "        The token to use as HTTP bearer authorization for remote files. If :obj:`True`, will use the token\n",
       "        generated when running :obj:`transformers-cli login` (stored in :obj:`~/.huggingface`).\n",
       "    revision(:obj:`str`, `optional`, defaults to :obj:`\"main\"`):\n",
       "        The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n",
       "        git-based system for storing models and other artifacts on huggingface.co, so ``revision`` can be any\n",
       "        identifier allowed by git.\n",
       "    mirror(:obj:`str`, `optional`):\n",
       "        Mirror source to accelerate downloads in China. If you are from China and have an accessibility\n",
       "        problem, you can set this option to resolve it. Note that we do not guarantee the timeliness or safety.\n",
       "        Please refer to the mirror site for more information.\n",
       "    _fast_init(:obj:`bool`, `optional`, defaults to `:obj:`True`):\n",
       "        Whether or not to disable fast initialization.\n",
       "\n",
       "        .. warning::\n",
       "\n",
       "            One should only disable `_fast_init` to ensure backwards compatibility with\n",
       "            ``transformers.__version__ < 4.6.0`` for seeded model initialization. This argument will be removed\n",
       "            at the next major version. See `pull request 11471\n",
       "            <https://github.com/huggingface/transformers/pull/11471>`__ for more information.\n",
       "\n",
       "    kwargs (remaining dictionary of keyword arguments, `optional`):\n",
       "        Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,\n",
       "        :obj:`output_attentions=True`). Behaves differently depending on whether a ``config`` is provided or\n",
       "        automatically loaded:\n",
       "\n",
       "            - If a configuration is provided with ``config``, ``**kwargs`` will be directly passed to the\n",
       "              underlying model's ``__init__`` method (we assume all relevant updates to the configuration have\n",
       "              already been done)\n",
       "            - If a configuration is not provided, ``kwargs`` will be first passed to the configuration class\n",
       "              initialization function (:func:`~transformers.PretrainedConfig.from_pretrained`). Each key of\n",
       "              ``kwargs`` that corresponds to a configuration attribute will be used to override said attribute\n",
       "              with the supplied ``kwargs`` value. Remaining keys that do not correspond to any configuration\n",
       "              attribute will be passed to the underlying model's ``__init__`` function.\n",
       "\n",
       ".. note::\n",
       "\n",
       "    Passing :obj:`use_auth_token=True` is required when you want to use a private model.\n",
       "\n",
       ".. note::\n",
       "\n",
       "    Activate the special `\"offline-mode\"\n",
       "    <https://huggingface.co/transformers/installation.html#offline-mode>`__ to use this method in a firewalled\n",
       "    environment.\n",
       "\n",
       "Examples::\n",
       "\n",
       "    >>> from transformers import BertConfig, BertModel\n",
       "    >>> # Download model and configuration from huggingface.co and cache.\n",
       "    >>> model = BertModel.from_pretrained('bert-base-uncased')\n",
       "    >>> # Model was saved using `save_pretrained('./test/saved_model/')` (for example purposes, not runnable).\n",
       "    >>> model = BertModel.from_pretrained('./test/saved_model/')\n",
       "    >>> # Update configuration during loading.\n",
       "    >>> model = BertModel.from_pretrained('bert-base-uncased', output_attentions=True)\n",
       "    >>> assert model.config.output_attentions == True\n",
       "    >>> # Loading from a TF checkpoint file instead of a PyTorch model (slower, for example purposes, not runnable).\n",
       "    >>> config = BertConfig.from_json_file('./tf_model/my_tf_model_config.json')\n",
       "    >>> model = BertModel.from_pretrained('./tf_model/my_tf_checkpoint.ckpt.index', from_tf=True, config=config)\n",
       "    >>> # Loading from a Flax checkpoint file instead of a PyTorch model (slower)\n",
       "    >>> model = BertModel.from_pretrained('bert-base-uncased', from_flax=True)\n",
       "\u001b[0;31mFile:\u001b[0m      ~/opt/anaconda3/envs/capstone/lib/python3.7/site-packages/transformers/modeling_utils.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?BertForSequenceClassification.from_pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b456275a-4350-4e16-bc5d-b01eee2b870b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load BerTweet tokenizer\n",
    "#TOKENIZER = AutoTokenizer.from_pretrained(\"finiteautomata/bertweet-base-sentiment-analysis\", normalization=True)\n",
    "\n",
    "TOKENIZER = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case= False, normalization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b01bdf83-cd51-411c-82aa-8f65685ac25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"ezraelani spectranet_ng they got different types....you can check their website or any of their store close to youðŸ˜ŠðŸ˜Š\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4ccbc41-8ee2-4117-8e25-7de340070be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = TOKENIZER.encode_plus(\n",
    "  text,\n",
    "  add_special_tokens=True,\n",
    "  max_length=TOKENIZER.model_max_length,\n",
    "  return_token_type_ids=False,\n",
    "  padding=\"max_length\",\n",
    "  return_attention_mask=True,\n",
    "  return_tensors='pt',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ace7f483-5303-4d2d-9921-662deefefb7e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BertTokenizer' object has no attribute 'normalize'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/j5/540q0bw12gx3g56qg4llbmlh0000gn/T/ipykernel_5829/579443411.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mTOKENIZER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'BertTokenizer' object has no attribute 'normalize'"
     ]
    }
   ],
   "source": [
    "TOKENIZER.normalize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8a5126a0-1369-499c-93c3-4413492f1490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 13610,\n",
       " 1127,\n",
       " 1091,\n",
       " 3080,\n",
       " 8703,\n",
       " 8766,\n",
       " 1828,\n",
       " 524,\n",
       " 2709,\n",
       " 59,\n",
       " 100,\n",
       " 589,\n",
       " 4594,\n",
       " 28,\n",
       " 14,\n",
       " 56,\n",
       " 560,\n",
       " 130,\n",
       " 1453,\n",
       " 72,\n",
       " 207,\n",
       " 15,\n",
       " 130,\n",
       " 1297,\n",
       " 743,\n",
       " 9,\n",
       " 14,\n",
       " 518,\n",
       " 518,\n",
       " 2]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOKENIZER.encode(TOKENIZER.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "284ee386-ae60-4b00-b285-a4a37cd75689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ezraelani spectranet_ng they got different types....you can check their website or any of their store close to youðŸ˜ŠðŸ˜Š'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "668c9afa-c04c-4fd9-883a-4a7a7615d588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    0, 13610,  1127,  1091,  3080,  8703,  8766,  1828,   524,  2709,\n",
       "           59,   100,   589,  4594,    28,    14,    56,   560,   130,  1453,\n",
       "           72,   207,    15,   130,  1297,   743,     9,    14,   518,   518,\n",
       "            2,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a220a61a-41a3-4781-bade-dc3adf98f6a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ezraelani spectranet_ng they got different types ... you can check their website or any of their store close to you :smiling_face_with_smiling_eyes: :smiling_face_with_smiling_eyes:'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOKENIZER.normalizeTweet(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d8658f9c-e80a-452c-8f9c-d11db1316e6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ez@@',\n",
       " 'ra@@',\n",
       " 'el@@',\n",
       " 'ani',\n",
       " 'spec@@',\n",
       " 'tran@@',\n",
       " 'et@@',\n",
       " '_@@',\n",
       " 'ng',\n",
       " 'they',\n",
       " 'got',\n",
       " 'different',\n",
       " 'types',\n",
       " '...',\n",
       " 'you',\n",
       " 'can',\n",
       " 'check',\n",
       " 'their',\n",
       " 'website',\n",
       " 'or',\n",
       " 'any',\n",
       " 'of',\n",
       " 'their',\n",
       " 'store',\n",
       " 'close',\n",
       " 'to',\n",
       " 'you',\n",
       " ':smiling_face_with_smiling_eyes:',\n",
       " ':smiling_face_with_smiling_eyes:']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOKENIZER.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3608ff13-7fd2-4598-b6b2-733f716a22d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mTOKENIZER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_plus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtext_pair\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpadding\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPaddingStrategy\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtruncation\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization_utils_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTruncationStrategy\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstride\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mis_split_into_words\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpad_to_multiple_of\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreturn_token_type_ids\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreturn_attention_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreturn_overflowing_tokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreturn_special_tokens_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreturn_offsets_mapping\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreturn_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization_utils_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBatchEncoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Tokenize and prepare for the model a sequence or a pair of sequences.\n",
       "\n",
       ".. warning::\n",
       "    This method is deprecated, ``__call__`` should be used instead.\n",
       "\n",
       "Args:\n",
       "    text (:obj:`str`, :obj:`List[str]` or :obj:`List[int]` (the latter only for not-fast tokenizers)):\n",
       "        The first sequence to be encoded. This can be a string, a list of strings (tokenized string using the\n",
       "        ``tokenize`` method) or a list of integers (tokenized string ids using the ``convert_tokens_to_ids``\n",
       "        method).\n",
       "    text_pair (:obj:`str`, :obj:`List[str]` or :obj:`List[int]`, `optional`):\n",
       "        Optional second sequence to be encoded. This can be a string, a list of strings (tokenized string using\n",
       "        the ``tokenize`` method) or a list of integers (tokenized string ids using the\n",
       "        ``convert_tokens_to_ids`` method).\n",
       "\n",
       "    add_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
       "        Whether or not to encode the sequences with the special tokens relative to their model.\n",
       "    padding (:obj:`bool`, :obj:`str` or :class:`~transformers.file_utils.PaddingStrategy`, `optional`, defaults to :obj:`False`):\n",
       "        Activates and controls padding. Accepts the following values:\n",
       "\n",
       "        * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a\n",
       "          single sequence if provided).\n",
       "        * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
       "          maximum acceptable input length for the model if that argument is not provided.\n",
       "        * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
       "          different lengths).\n",
       "    truncation (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.TruncationStrategy`, `optional`, defaults to :obj:`False`):\n",
       "        Activates and controls truncation. Accepts the following values:\n",
       "\n",
       "        * :obj:`True` or :obj:`'longest_first'`: Truncate to a maximum length specified with the argument\n",
       "          :obj:`max_length` or to the maximum acceptable input length for the model if that argument is not\n",
       "          provided. This will truncate token by token, removing a token from the longest sequence in the pair\n",
       "          if a pair of sequences (or a batch of pairs) is provided.\n",
       "        * :obj:`'only_first'`: Truncate to a maximum length specified with the argument :obj:`max_length` or to\n",
       "          the maximum acceptable input length for the model if that argument is not provided. This will only\n",
       "          truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
       "        * :obj:`'only_second'`: Truncate to a maximum length specified with the argument :obj:`max_length` or\n",
       "          to the maximum acceptable input length for the model if that argument is not provided. This will only\n",
       "          truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
       "        * :obj:`False` or :obj:`'do_not_truncate'` (default): No truncation (i.e., can output batch with\n",
       "          sequence lengths greater than the model maximum admissible input size).\n",
       "    max_length (:obj:`int`, `optional`):\n",
       "        Controls the maximum length to use by one of the truncation/padding parameters.\n",
       "\n",
       "        If left unset or set to :obj:`None`, this will use the predefined model maximum length if a maximum\n",
       "        length is required by one of the truncation/padding parameters. If the model has no specific maximum\n",
       "        input length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
       "    stride (:obj:`int`, `optional`, defaults to 0):\n",
       "        If set to a number along with :obj:`max_length`, the overflowing tokens returned when\n",
       "        :obj:`return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
       "        returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
       "        argument defines the number of overlapping tokens.\n",
       "    is_split_into_words (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
       "        Whether or not the input is already pre-tokenized (e.g., split into words). If set to :obj:`True`, the\n",
       "        tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n",
       "        which it will tokenize. This is useful for NER or token classification.\n",
       "    pad_to_multiple_of (:obj:`int`, `optional`):\n",
       "        If set will pad the sequence to a multiple of the provided value. This is especially useful to enable\n",
       "        the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n",
       "    return_tensors (:obj:`str` or :class:`~transformers.file_utils.TensorType`, `optional`):\n",
       "        If set, will return tensors instead of list of python integers. Acceptable values are:\n",
       "\n",
       "        * :obj:`'tf'`: Return TensorFlow :obj:`tf.constant` objects.\n",
       "        * :obj:`'pt'`: Return PyTorch :obj:`torch.Tensor` objects.\n",
       "        * :obj:`'np'`: Return Numpy :obj:`np.ndarray` objects.\n",
       "\n",
       "    return_token_type_ids (:obj:`bool`, `optional`):\n",
       "        Whether to return token type IDs. If left to the default, will return the token type IDs according to\n",
       "        the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n",
       "\n",
       "        `What are token type IDs? <../glossary.html#token-type-ids>`__\n",
       "    return_attention_mask (:obj:`bool`, `optional`):\n",
       "        Whether to return the attention mask. If left to the default, will return the attention mask according\n",
       "        to the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n",
       "\n",
       "        `What are attention masks? <../glossary.html#attention-mask>`__\n",
       "    return_overflowing_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
       "        Whether or not to return overflowing token sequences.\n",
       "    return_special_tokens_mask (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
       "        Whether or not to return special tokens mask information.\n",
       "    return_offsets_mapping (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
       "        Whether or not to return :obj:`(char_start, char_end)` for each token.\n",
       "\n",
       "        This is only available on fast tokenizers inheriting from\n",
       "        :class:`~transformers.PreTrainedTokenizerFast`, if using Python's tokenizer, this method will raise\n",
       "        :obj:`NotImplementedError`.\n",
       "    return_length  (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
       "        Whether or not to return the lengths of the encoded inputs.\n",
       "    verbose (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
       "        Whether or not to print more information and warnings.\n",
       "    **kwargs: passed to the :obj:`self.tokenize()` method\n",
       "\n",
       "Return:\n",
       "    :class:`~transformers.BatchEncoding`: A :class:`~transformers.BatchEncoding` with the following fields:\n",
       "\n",
       "    - **input_ids** -- List of token ids to be fed to a model.\n",
       "\n",
       "      `What are input IDs? <../glossary.html#input-ids>`__\n",
       "\n",
       "    - **token_type_ids** -- List of token type ids to be fed to a model (when :obj:`return_token_type_ids=True`\n",
       "      or if `\"token_type_ids\"` is in :obj:`self.model_input_names`).\n",
       "\n",
       "      `What are token type IDs? <../glossary.html#token-type-ids>`__\n",
       "\n",
       "    - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
       "      :obj:`return_attention_mask=True` or if `\"attention_mask\"` is in :obj:`self.model_input_names`).\n",
       "\n",
       "      `What are attention masks? <../glossary.html#attention-mask>`__\n",
       "\n",
       "    - **overflowing_tokens** -- List of overflowing tokens sequences (when a :obj:`max_length` is specified and\n",
       "      :obj:`return_overflowing_tokens=True`).\n",
       "    - **num_truncated_tokens** -- Number of tokens truncated (when a :obj:`max_length` is specified and\n",
       "      :obj:`return_overflowing_tokens=True`).\n",
       "    - **special_tokens_mask** -- List of 0s and 1s, with 1 specifying added special tokens and 0 specifying\n",
       "      regular sequence tokens (when :obj:`add_special_tokens=True` and :obj:`return_special_tokens_mask=True`).\n",
       "    - **length** -- The length of the inputs (when :obj:`return_length=True`)\n",
       "\u001b[0;31mFile:\u001b[0m      ~/opt/anaconda3/envs/capstone/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?TOKENIZER.encode_plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44061db6-83dc-454c-a651-ac9c87dff42b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
